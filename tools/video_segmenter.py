#!/usr/bin/env python3
"""
Optical Flow ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ì„¸ê·¸ë©˜í…Œì´ì…˜
ê²Œì„ ì˜ìƒì„ ì›€ì§ì„ íŒ¨í„´ì— ë”°ë¼ ë¶„í• í•˜ì—¬ YOLO í•™ìŠµ ë°ì´í„° ìµœì í™”

ì»¤íŒ… ì „ëµ:
- ì›€ì§ì„ ì ì€ êµ¬ê°„: ë²„ë¦¼ (ì˜¤ë²„í”¼íŒ… ë°©ì§€)
- ì¥ë©´ ì „í™˜ êµ¬ê°„: ìš°ì„  ì„ íƒ (ì»¨í…ì¸  ì…ì¥/ì¢…ë£Œ ê°ì§€)
- UI ê³ ì • + ë°°ê²½ ë™ì  êµ¬ê°„: ì ë‹¹ëŸ‰ ì„ íƒ (UI ì¶”ì  í•™ìŠµ)

ì‚¬ìš©ë²•:
    python tools/video_segmenter.py --input datasets/raw/gameplay.mp4 \
                                     --output datasets/clips/ \
                                     --motion-low 2.0 \
                                     --motion-high 15.0 \
                                     --min-duration 5
"""

import argparse
import cv2
import numpy as np
from pathlib import Path
from typing import List, Tuple, Optional
from dataclasses import dataclass
import json
from datetime import datetime
import subprocess
import shutil
import sys
import os


def refresh_system_path():
    """
    ì‹œìŠ¤í…œ PATH í™˜ê²½ë³€ìˆ˜ë¥¼ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ìƒˆë¡œê³ ì¹¨ (Windows ì „ìš©)
    winget ì„¤ì¹˜ í›„ í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì—ì„œ PATHë¥¼ ì¦‰ì‹œ ì‚¬ìš©í•˜ê¸° ìœ„í•¨
    """
    if sys.platform != 'win32':
        return

    try:
        import winreg
        import os

        # ì‹œìŠ¤í…œ PATH ì½ê¸°
        with winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE,
                           r'SYSTEM\CurrentControlSet\Control\Session Manager\Environment',
                           0, winreg.KEY_READ) as key:
            system_path, _ = winreg.QueryValueEx(key, 'Path')

        # ì‚¬ìš©ì PATH ì½ê¸°
        with winreg.OpenKey(winreg.HKEY_CURRENT_USER,
                           r'Environment',
                           0, winreg.KEY_READ) as key:
            try:
                user_path, _ = winreg.QueryValueEx(key, 'Path')
            except FileNotFoundError:
                user_path = ''

        # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ PATH ì—…ë°ì´íŠ¸
        combined_path = f"{user_path};{system_path}" if user_path else system_path
        os.environ['PATH'] = combined_path

    except Exception as e:
        # PATH ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ë¬´ì‹œ
        pass


def check_and_install_ffmpeg() -> bool:
    """
    ffmpeg ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸ ë° ìë™ ì„¤ì¹˜

    Returns:
        bool: ffmpegê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ True, ì„¤ì¹˜ ì‹¤íŒ¨í•˜ë©´ False
    """
    # ffmpegê°€ ì´ë¯¸ PATHì— ìˆëŠ”ì§€ í™•ì¸
    if shutil.which('ffmpeg') is not None:
        return True

    print("âš ï¸ ffmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ğŸ”§ ffmpegë¥¼ ìë™ìœ¼ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤ (winget ì‚¬ìš©)...")

    try:
        # wingetìœ¼ë¡œ ffmpeg ì„¤ì¹˜ ì‹œë„
        result = subprocess.run(
            ['winget', 'install', 'Gyan.FFmpeg', '--accept-source-agreements', '--accept-package-agreements'],
            capture_output=True,
            text=True,
            timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
        )

        if result.returncode == 0:
            print("âœ… ffmpeg ì„¤ì¹˜ ì™„ë£Œ!")

            # PATH ìƒˆë¡œê³ ì¹¨ ì‹œë„
            print("ğŸ”„ PATH í™˜ê²½ë³€ìˆ˜ ìƒˆë¡œê³ ì¹¨ ì¤‘...")
            refresh_system_path()

            # ì„¤ì¹˜ í›„ ë‹¤ì‹œ í™•ì¸
            if shutil.which('ffmpeg') is not None:
                print("âœ… ffmpegë¥¼ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
                return True
            else:
                print("   âš ï¸ ì„¤ì¹˜ëŠ” ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ PATHì—ì„œ ffmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("   â„¹ï¸ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì‹œë„í•´ì£¼ì„¸ìš”:")
                print("      1. í„°ë¯¸ë„ì„ ì¬ì‹œì‘í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰")
                print("      2. ì‹œìŠ¤í…œì„ ì¬ë¶€íŒ…")
                return False
        else:
            print(f"âŒ ffmpeg ì„¤ì¹˜ ì‹¤íŒ¨")
            if result.stderr:
                print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {result.stderr[:200]}")
            print("   â„¹ï¸ ìˆ˜ë™ ì„¤ì¹˜ ë°©ë²•:")
            print("      1. í„°ë¯¸ë„ì—ì„œ 'winget install Gyan.FFmpeg' ì‹¤í–‰")
            print("      2. ë˜ëŠ” https://www.gyan.dev/ffmpeg/builds/ ì—ì„œ ë‹¤ìš´ë¡œë“œ")
            return False

    except FileNotFoundError:
        print("âŒ wingetì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   â„¹ï¸ Windows 10 1809 ì´ìƒ ë˜ëŠ” Windows 11ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   â„¹ï¸ ìˆ˜ë™ ì„¤ì¹˜: https://www.gyan.dev/ffmpeg/builds/")
        return False
    except subprocess.TimeoutExpired:
        print("âŒ ffmpeg ì„¤ì¹˜ ì‹œê°„ ì´ˆê³¼ (5ë¶„)")
        print("   â„¹ï¸ ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return False
    except Exception as e:
        print(f"âŒ ffmpeg ì„¤ì¹˜ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return False


class PyAVVideoReader:
    """
    PyAVë¥¼ ì‚¬ìš©í•œ ë¹„ë””ì˜¤ ë¦¬ë” (OpenCVê°€ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì½”ë± ì²˜ë¦¬)
    AV1, H.265, VP9 ë“± ëª¨ë“  FFmpeg ì§€ì› ì½”ë±ì„ ë¬´ì†ì‹¤ë¡œ ì½ì„ ìˆ˜ ìˆìŒ
    """

    def __init__(self, video_path: Path):
        """
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        self.video_path = video_path
        self.container = None
        self.video_stream = None
        self.fps = None
        self.total_frames = None
        self.width = None
        self.height = None
        self._frame_generator = None

    def open(self) -> bool:
        """
        ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸°

        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            import av

            self.container = av.open(str(self.video_path))
            self.video_stream = self.container.streams.video[0]

            # ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ
            self.fps = float(self.video_stream.average_rate)
            self.total_frames = self.video_stream.frames
            self.width = self.video_stream.width
            self.height = self.video_stream.height

            # total_framesê°€ 0ì´ë©´ durationìœ¼ë¡œ ì¶”ì •
            if self.total_frames == 0 and self.container.duration:
                self.total_frames = int(self.container.duration * self.fps / av.time_base)

            # í”„ë ˆì„ ì œë„ˆë ˆì´í„° ì´ˆê¸°í™”
            self._frame_generator = self.container.decode(video=0)

            print(f"âœ… PyAVë¡œ ë¹„ë””ì˜¤ ì—´ê¸° ì„±ê³µ")
            print(f"   ì½”ë±: {self.video_stream.codec_context.name}")

            return True

        except ImportError:
            print("âš ï¸ PyAVê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ì„¤ì¹˜ ë°©ë²•: pip install av")
            return False
        except Exception as e:
            print(f"âš ï¸ PyAVë¡œ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {e}")
            return False

    def read(self):
        """
        ë‹¤ìŒ í”„ë ˆì„ ì½ê¸° (OpenCVì˜ cap.read()ì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)

        Returns:
            tuple: (success, frame_bgr) - OpenCV í˜•ì‹ì˜ BGR numpy array
        """
        try:
            frame = next(self._frame_generator)
            # BGR í¬ë§·ìœ¼ë¡œ ë³€í™˜ (OpenCV í˜¸í™˜)
            img = frame.to_ndarray(format='bgr24')
            return True, img
        except StopIteration:
            return False, None
        except Exception as e:
            return False, None

    def grab(self):
        """
        í”„ë ˆì„ì„ ê±´ë„ˆë›°ê¸° (OpenCVì˜ cap.grab()ì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)
        PyAVëŠ” grabì„ ì§ì ‘ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ readí•˜ê³  ë²„ë¦¼

        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            frame = next(self._frame_generator)
            return True
        except StopIteration:
            return False
        except Exception as e:
            return False

    def release(self):
        """ë¦¬ì†ŒìŠ¤ í•´ì œ"""
        if self.container:
            self.container.close()
            self.container = None

    def isOpened(self) -> bool:
        """ë¹„ë””ì˜¤ê°€ ì—´ë ¤ìˆëŠ”ì§€ í™•ì¸"""
        return self.container is not None


@dataclass
class VideoSegment:
    """ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´"""
    start_frame: int
    end_frame: int
    start_time: float
    end_time: float
    duration: float
    avg_motion: float  # êµ¬ê°„ ë‚´ í‰ê·  ì›€ì§ì„ í¬ê¸°
    avg_scene_change: float  # êµ¬ê°„ ë‚´ í‰ê·  ì¥ë©´ ì „í™˜ ì ìˆ˜
    priority: int  # ìš°ì„ ìˆœìœ„ (1=ì¥ë©´ì „í™˜, 2=ì¤‘ê°„ë™ì , 3=ì €ë™ì )


@dataclass
class SegmentConfig:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì •"""
    # Optical Flow ê¸°ë°˜ ì›€ì§ì„ ê°ì§€
    motion_low_threshold: float = 2.0    # ì´ë³´ë‹¤ ë‚®ìœ¼ë©´ ì €ë™ì  êµ¬ê°„ (ë²„ë¦¼)
    motion_high_threshold: float = 15.0  # ì´ë³´ë‹¤ ë†’ìœ¼ë©´ ê³¼ë„í•œ ì›€ì§ì„ (ì œì™¸)

    # ì¥ë©´ ì „í™˜ ê°ì§€ (íˆìŠ¤í† ê·¸ë¨ ì°¨ì´)
    scene_change_threshold: float = 0.5  # íˆìŠ¤í† ê·¸ë¨ ì°¨ì´ê°€ ì´ë³´ë‹¤ í¬ë©´ ì¥ë©´ ì „í™˜
    scene_change_important: float = 0.5  # ì¥ë©´ ì „í™˜ ì ìˆ˜ê°€ ì´ë³´ë‹¤ ë†’ìœ¼ë©´ ìš°ì„  ì„ íƒ

    # ì„¸ê·¸ë¨¼íŠ¸ ë¶„ë¥˜ ê¸°ì¤€
    min_dynamic_frames: int = 30         # ìµœì†Œ ë™ì  í”„ë ˆì„ ìˆ˜ (1ì´ˆ@30fps)

    # ì„¸ê·¸ë¨¼íŠ¸ ì œì•½
    min_duration: float = 5.0            # ìµœì†Œ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)
    max_duration: float = 60.0           # ìµœëŒ€ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)
    max_segments: Optional[int] = None   # ìµœëŒ€ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜

    # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì„ íƒ
    priority_ratios: dict = None         # {1: ë¹„ìœ¨, 2: ë¹„ìœ¨, 3: ë¹„ìœ¨} ì˜ˆ: {1: 0.4, 2: 0.5, 3: 0.1}

    # ì„±ëŠ¥ ìµœì í™”
    flow_scale: float = 0.5              # Optical Flow ê³„ì‚° ì‹œ í•´ìƒë„ ìŠ¤ì¼€ì¼ (0.5 = 2ë°° ë¹ ë¦„)
    frame_skip: int = 1                  # í”„ë ˆì„ ìŠ¤í‚µ (1=ëª¨ë“  í”„ë ˆì„, 3=3í”„ë ˆì„ë§ˆë‹¤)
    use_gpu: bool = False                # GPU ê°€ì† ì‚¬ìš© (CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ)

    # ì‹¤í—˜ ê¸°ëŠ¥
    save_discarded: bool = False         # ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ë„ ë³„ë„ ì €ì¥

    # ì¶œë ¥ ì„¤ì •
    output_codec: str = "mp4v"           # ì¶œë ¥ ì½”ë±
    output_fps: Optional[int] = None     # ì¶œë ¥ FPS (Noneì´ë©´ ì›ë³¸)

    def __post_init__(self):
        if self.priority_ratios is None:
            self.priority_ratios = {1: 0.4, 2: 0.5, 3: 0.1}  # ê¸°ë³¸ê°’


class VideoSegmenter:
    """Optical Flow ê¸°ë°˜ ë¹„ë””ì˜¤ ì„¸ê·¸ë©˜í…Œì´ì…˜"""

    def __init__(self, config: SegmentConfig = None):
        self.config = config or SegmentConfig()
        self.stats = {
            'total_frames': 0,
            'scene_changes': 0,
            'priority_1_segments': 0,  # ì¥ë©´ ì „í™˜ í¬í•¨ ì„¸ê·¸ë¨¼íŠ¸
            'priority_2_segments': 0,  # ì¤‘ê°„ ë™ì  ì„¸ê·¸ë¨¼íŠ¸
            'priority_3_segments': 0,  # ì €ë™ì  ì„¸ê·¸ë¨¼íŠ¸
            'discarded_short': 0,
            'discarded_low_motion': 0,
            'flow_gpu_count': 0,       # GPUë¡œ ê³„ì‚°í•œ Optical Flow íšŸìˆ˜
            'flow_cpu_count': 0,       # CPUë¡œ ê³„ì‚°í•œ Optical Flow íšŸìˆ˜
            'flow_gpu_time': 0.0,      # GPU Flow ì´ ì‹œê°„ (ì´ˆ)
            'flow_cpu_time': 0.0,      # CPU Flow ì´ ì‹œê°„ (ì´ˆ)
        }

        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.gpu_available = False
        self.device = None
        self.sobel_x = None  # Sobel X í•„í„° (ì¬ì‚¬ìš©)
        self.sobel_y = None  # Sobel Y í•„í„° (ì¬ì‚¬ìš©)
        if self.config.use_gpu:
            self.gpu_available = self._check_gpu_available()

    def _check_gpu_available(self) -> bool:
        """
        GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (CUDA/PyTorch)

        Returns:
            bool: GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ True
        """
        try:
            # 1. PyTorch ì„¤ì¹˜ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€
            import sys
            from pathlib import Path
            import os

            # src/utils ê²½ë¡œ ì¶”ê°€ (PyTorchInstaller importìš©)
            if getattr(sys, 'frozen', False):
                # PyInstaller íŒ¨í‚¤ì§• í™˜ê²½
                utils_dir = Path(sys.executable).parent / "_internal" / "src"
            else:
                # ê°œë°œ í™˜ê²½
                script_dir = Path(__file__).parent.parent
                utils_dir = script_dir / "src"

            if utils_dir.exists() and str(utils_dir) not in sys.path:
                sys.path.insert(0, str(utils_dir))

            # 2. PyTorch ì„¤ì¹˜ í™•ì¸
            try:
                from utils.pytorch_installer import PyTorchInstaller
                installer = PyTorchInstaller.get_instance()

                if not installer.is_pytorch_installed():
                    print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    print("   GPU ê°€ì†ì„ ì‚¬ìš©í•˜ë ¤ë©´ GUIì—ì„œ 'GPU ê°€ì†' ì²´í¬ë°•ìŠ¤ë¥¼ í™œì„±í™”í•˜ì„¸ìš”.")
                    return False

                # sys.pathì— PyTorch ê²½ë¡œ ì¶”ê°€
                installer.add_to_path()

            except ImportError:
                # PyTorchInstallerë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° (ì´ì „ ë²„ì „ í˜¸í™˜ì„±)
                print("âš ï¸ PyTorchInstallerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ PyTorchë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

            # 3. PyTorch import ì‹œë„
            import torch

            if torch.cuda.is_available():
                self.device = torch.device('cuda')
                gpu_name = torch.cuda.get_device_name(0)
                print(f"âœ… GPU ê°ì§€ë¨: {gpu_name}")

                # 4. ì‹¤ì œ GPU í…ì„œ ìƒì„± ë° ì—°ì‚° í…ŒìŠ¤íŠ¸
                print("ğŸ” GPU ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì¤‘...")
                try:
                    # ì‘ì€ í–‰ë ¬ ê³±ì…ˆìœ¼ë¡œ GPU ì‘ë™ í™•ì¸
                    test_tensor = torch.randn(100, 100, device=self.device)
                    result = test_tensor @ test_tensor.T
                    torch.cuda.synchronize()  # GPU ì‘ì—… ì™„ë£Œ ëŒ€ê¸°

                    # ë©”ëª¨ë¦¬ ì •ë³´ í™•ì¸
                    memory_allocated = torch.cuda.memory_allocated(0) / 1024 / 1024  # MB
                    memory_reserved = torch.cuda.memory_reserved(0) / 1024 / 1024    # MB

                    print(f"âœ… GPU ê°€ì† í™œì„±í™” ì„±ê³µ!")
                    print(f"   - GPU ë©”ëª¨ë¦¬ í• ë‹¹: {memory_allocated:.1f} MB")
                    print(f"   - GPU ë©”ëª¨ë¦¬ ì˜ˆì•½: {memory_reserved:.1f} MB")

                    # Sobel í•„í„° ë¯¸ë¦¬ ìƒì„± (GPU ë©”ëª¨ë¦¬ì— ìƒì£¼)
                    self.sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]],
                                               dtype=torch.float32, device=self.device).view(1, 1, 3, 3)
                    self.sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]],
                                               dtype=torch.float32, device=self.device).view(1, 1, 3, 3)
                    print("âœ… GPU Sobel í•„í„° ì´ˆê¸°í™” ì™„ë£Œ")

                    return True

                except RuntimeError as e:
                    print(f"âŒ GPU í…ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
                    print("   CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    return False
                except Exception as e:
                    print(f"âŒ GPU ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                    print("   CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    return False
            else:
                print("âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                return False

        except ImportError:
            print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            print("   GPU ê°€ì†ì„ ì‚¬ìš©í•˜ë ¤ë©´ GUIì—ì„œ 'GPU ê°€ì†' ì²´í¬ë°•ìŠ¤ë¥¼ í™œì„±í™”í•˜ì„¸ìš”.")
            return False
        except (OSError, RuntimeError) as e:
            # DLL ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” CUDA ì´ˆê¸°í™” ì˜¤ë¥˜
            print(f"âš ï¸ PyTorch ë¡œë”© ì‹¤íŒ¨: {e}")
            print("   CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            print("   í•´ê²° ë°©ë²•:")
            print("   1. GUIì—ì„œ 'GPU ê°€ì†' ì²´í¬ë°•ìŠ¤ë¥¼ ë‹¤ì‹œ í™œì„±í™”í•˜ì„¸ìš”.")
            print("   2. PyTorch ì¬ì„¤ì¹˜: ì„¤ì • ë©”ë‰´ì—ì„œ 'PyTorch ì¬ì„¤ì¹˜' í´ë¦­")
            return False
        except Exception as e:
            # ê¸°íƒ€ ëª¨ë“  ì˜ˆì™¸
            print(f"âš ï¸ GPU ì´ˆê¸°í™” ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            print("   CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            return False

    def calculate_motion_and_scene_change(
        self,
        prev_frame: np.ndarray,
        current_frame: np.ndarray
    ) -> Tuple[float, float]:
        """
        Optical Flow ê¸°ë°˜ ì›€ì§ì„ í¬ê¸° + íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ì¥ë©´ ì „í™˜ ì ìˆ˜ ê³„ì‚°

        Returns:
            (motion_score, scene_change_score)
            - motion_score: í‰ê·  optical flow í¬ê¸° (í”½ì…€/í”„ë ˆì„)
            - scene_change_score: íˆìŠ¤í† ê·¸ë¨ ì°¨ì´ (0~1, 1ì´ ì™„ì „íˆ ë‹¤ë¦„)
        """
        import time

        # í•´ìƒë„ ì¶•ì†Œ (ì„¤ì •ëœ ê²½ìš°)
        if self.config.flow_scale < 1.0:
            h, w = prev_frame.shape[:2]
            new_h = int(h * self.config.flow_scale)
            new_w = int(w * self.config.flow_scale)
            prev_small = cv2.resize(prev_frame, (new_w, new_h), interpolation=cv2.INTER_AREA)
            curr_small = cv2.resize(current_frame, (new_w, new_h), interpolation=cv2.INTER_AREA)
        else:
            prev_small = prev_frame
            curr_small = current_frame

        # GPU ê°€ì† ì‚¬ìš© (PyTorch + CUDA Optical Flow)
        if self.gpu_available:
            try:
                start_time = time.perf_counter()
                motion_score, scene_change_score = self._calculate_flow_gpu(prev_small, curr_small)
                elapsed = time.perf_counter() - start_time

                self.stats['flow_gpu_count'] += 1
                self.stats['flow_gpu_time'] += elapsed
                return motion_score, scene_change_score
            except (OSError, RuntimeError, Exception) as e:
                print(f"âš ï¸ GPU Optical Flow ê³„ì‚° ì‹¤íŒ¨, CPUë¡œ ì „í™˜: {e}")
                self.gpu_available = False

        # CPU ë²„ì „ (OpenCV Farneback)
        start_time = time.perf_counter()

        # 1. Optical Flow ê³„ì‚°
        prev_gray = cv2.cvtColor(prev_small, cv2.COLOR_BGR2GRAY)
        curr_gray = cv2.cvtColor(curr_small, cv2.COLOR_BGR2GRAY)

        flow = cv2.calcOpticalFlowFarneback(
            prev_gray, curr_gray,
            None,
            pyr_scale=0.5,
            levels=3,
            winsize=15,
            iterations=3,
            poly_n=5,
            poly_sigma=1.2,
            flags=0
        )

        # ì›€ì§ì„ í¬ê¸° (magnitude)
        magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)
        motion_score = np.mean(magnitude)

        # 2. íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ì¥ë©´ ì „í™˜ ê°ì§€
        hist_prev = cv2.calcHist([prev_small], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
        hist_curr = cv2.calcHist([curr_small], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])

        hist_prev = cv2.normalize(hist_prev, hist_prev).flatten()
        hist_curr = cv2.normalize(hist_curr, hist_curr).flatten()

        # Correlation ê¸°ë°˜ ìœ ì‚¬ë„ (1 - correlation = ì°¨ì´)
        scene_change_score = 1.0 - cv2.compareHist(hist_prev, hist_curr, cv2.HISTCMP_CORREL)

        elapsed = time.perf_counter() - start_time
        self.stats['flow_cpu_count'] += 1
        self.stats['flow_cpu_time'] += elapsed

        return motion_score, scene_change_score

    def _calculate_flow_gpu(
        self,
        prev_frame: np.ndarray,
        current_frame: np.ndarray
    ) -> Tuple[float, float]:
        """
        GPUë¥¼ ì‚¬ìš©í•œ Optical Flow + íˆìŠ¤í† ê·¸ë¨ ê³„ì‚° (PyTorch ì™„ì „ GPU êµ¬í˜„)

        Lucas-Kanade ë°©ì‹ì˜ ê°„ì†Œí™”ëœ gradient-based optical flowë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        CPU Farnebackë³´ë‹¤ ì •í™•ë„ëŠ” ë‹¤ì†Œ ë‚®ì§€ë§Œ 20-50ë°° ë¹ ë¦…ë‹ˆë‹¤.

        Args:
            prev_frame: ì´ì „ í”„ë ˆì„ (BGR)
            current_frame: í˜„ì¬ í”„ë ˆì„ (BGR)

        Returns:
            (motion_score, scene_change_score)
        """
        try:
            import torch
            import torch.nn.functional as F

            # BGR -> Grayscale ë³€í™˜ (GPUì—ì„œ)
            # ê°€ì¤‘ì¹˜: B=0.114, G=0.587, R=0.299
            prev_tensor = torch.from_numpy(prev_frame).float().to(self.device)
            curr_tensor = torch.from_numpy(current_frame).float().to(self.device)

            prev_gray = (prev_tensor[..., 0] * 0.114 +
                        prev_tensor[..., 1] * 0.587 +
                        prev_tensor[..., 2] * 0.299)
            curr_gray = (curr_tensor[..., 0] * 0.114 +
                        curr_tensor[..., 1] * 0.587 +
                        curr_tensor[..., 2] * 0.299)

            # Add batch and channel dimensions for conv2d: [H, W] -> [1, 1, H, W]
            prev_gray = prev_gray.unsqueeze(0).unsqueeze(0)
            curr_gray = curr_gray.unsqueeze(0).unsqueeze(0)

            # Spatial gradients (Ix, Iy) - ë¯¸ë¦¬ ìƒì„±ëœ Sobel í•„í„° ì‚¬ìš©
            Ix = F.conv2d(curr_gray, self.sobel_x, padding=1)
            Iy = F.conv2d(curr_gray, self.sobel_y, padding=1)

            # Temporal gradient (It)
            It = curr_gray - prev_gray

            # Lucas-Kanade ë°©ì •ì‹: Ix*u + Iy*v + It = 0
            # ê°„ì†Œí™”ëœ ì¶”ì •: motion magnitude â‰ˆ |It| / (|Ix| + |Iy| + epsilon)
            # ë” ì •í™•í•œ ë°©ë²•ë„ ê°€ëŠ¥í•˜ì§€ë§Œ ì†ë„ë¥¼ ìœ„í•´ ë‹¨ìˆœí™”
            gradient_mag = torch.sqrt(Ix**2 + Iy**2) + 1e-6
            motion_magnitude = torch.abs(It) / gradient_mag

            # Motion score ê³„ì‚°
            motion_score = float(motion_magnitude.mean().cpu().item())

            # íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ì¥ë©´ ì „í™˜ ê°ì§€ (GPU)
            # 8x8x8 binsë¡œ RGB íˆìŠ¤í† ê·¸ë¨ ê³„ì‚°
            prev_rgb = prev_tensor / 32.0  # [0, 255] -> [0, 8) bins
            curr_rgb = curr_tensor / 32.0

            # Flatten spatial dimensions
            prev_flat = prev_rgb.view(-1, 3).long()
            curr_flat = curr_rgb.view(-1, 3).long()

            # Compute 3D histogram indices
            prev_indices = prev_flat[:, 0] * 64 + prev_flat[:, 1] * 8 + prev_flat[:, 2]
            curr_indices = curr_flat[:, 0] * 64 + curr_flat[:, 1] * 8 + curr_flat[:, 2]

            # Bincount (histogram)
            hist_prev = torch.bincount(prev_indices.clamp(0, 511), minlength=512).float()
            hist_curr = torch.bincount(curr_indices.clamp(0, 511), minlength=512).float()

            # Normalize
            hist_prev = hist_prev / (hist_prev.sum() + 1e-6)
            hist_curr = hist_curr / (hist_curr.sum() + 1e-6)

            # Correlation
            correlation = torch.sum(hist_prev * hist_curr) / (
                torch.sqrt(torch.sum(hist_prev**2)) * torch.sqrt(torch.sum(hist_curr**2)) + 1e-6
            )
            scene_change_score = float((1.0 - correlation).cpu().item())

            # GPU ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œ
            del prev_tensor, curr_tensor, prev_gray, curr_gray
            del Ix, Iy, It, gradient_mag, motion_magnitude
            del prev_rgb, curr_rgb, prev_flat, curr_flat
            del prev_indices, curr_indices, hist_prev, hist_curr, correlation

            return motion_score, scene_change_score

        except Exception as e:
            # GPU ì˜¤ë¥˜ ì‹œ CPUë¡œ í´ë°±
            print(f"âš ï¸ GPU Flow ê³„ì‚° ì‹¤íŒ¨, CPUë¡œ ì „í™˜: {e}")
            self.gpu_available = False  # GPU ë¹„í™œì„±í™”

            prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
            curr_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)

            flow = cv2.calcOpticalFlowFarneback(
                prev_gray, curr_gray, None,
                pyr_scale=0.5, levels=3, winsize=15,
                iterations=3, poly_n=5, poly_sigma=1.2, flags=0
            )

            magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)
            motion_score = np.mean(magnitude)

            hist_prev = cv2.calcHist([prev_frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
            hist_curr = cv2.calcHist([current_frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
            hist_prev = cv2.normalize(hist_prev, hist_prev).flatten()
            hist_curr = cv2.normalize(hist_curr, hist_curr).flatten()
            scene_change_score = 1.0 - cv2.compareHist(hist_prev, hist_curr, cv2.HISTCMP_CORREL)

            return motion_score, scene_change_score

    def detect_segments(
        self,
        video_path: Path,
        progress_callback=None
    ) -> List[VideoSegment]:
        """
        ë¹„ë””ì˜¤ì—ì„œ ë™ì ì¸ ì„¸ê·¸ë¨¼íŠ¸ íƒì§€ (ì •ì ì¸ 'ì ìˆ˜ êµ¬ê°„'ë§Œ ì œì™¸)

        Args:
            video_path: ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜(current, total)

        Returns:
            VideoSegment ë¦¬ìŠ¤íŠ¸
        """
        return self._detect_segments_single(video_path, progress_callback)

    def _detect_segments_single(
        self,
        video_path: Path,
        progress_callback=None
    ) -> List[VideoSegment]:
        """
        ì‹±ê¸€ í”„ë¡œì„¸ìŠ¤ ì„¸ê·¸ë¨¼íŠ¸ íƒì§€

        Args:
            video_path: ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜(current, total)

        Returns:
            VideoSegment ë¦¬ìŠ¤íŠ¸
        """
        # ë¹„ë””ì˜¤ ë¦¬ë” (OpenCV ë˜ëŠ” PyAV)
        cap = None
        using_pyav = False

        # 1ë‹¨ê³„: OpenCVë¡œ ì—´ê¸° ì‹œë„
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise RuntimeError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")

        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.stats['total_frames'] = total_frames

        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ë¶„ì„ ì¤‘...")
        print(f"   - FPS: {fps:.2f}")
        print(f"   - ì´ í”„ë ˆì„: {total_frames:,}ê°œ")
        print(f"   - ê¸¸ì´: {total_frames / fps / 60:.1f}ë¶„")
        if self.config.flow_scale < 1.0:
            print(f"   - Optical Flow í•´ìƒë„ ìŠ¤ì¼€ì¼: {self.config.flow_scale:.2f} (ì„±ëŠ¥ ìµœì í™” ì ìš©, ì¶œë ¥ì€ ì›ë³¸ ìœ ì§€)")
        if self.config.frame_skip > 1:
            print(f"   - í”„ë ˆì„ ìŠ¤í‚µ: {self.config.frame_skip} (ë¹ ë¥¸ ëª¨ë“œ, ~{self.config.frame_skip}ë°° ì†ë„ í–¥ìƒ)")

        segments = []
        current_segment_start = 0
        dynamic_frame_count = 0
        motion_buffer = []  # ì›€ì§ì„ ì ìˆ˜ ë²„í¼
        scene_change_buffer = []  # ì¥ë©´ ì „í™˜ ì ìˆ˜ ë²„í¼

        # ì²« í”„ë ˆì„ ì½ê¸° ì‹œë„
        ret, prev_frame = cap.read()
        if not ret:
            # 2ë‹¨ê³„: OpenCV ì‹¤íŒ¨ ì‹œ PyAVë¡œ ì „í™˜
            print("âš ï¸ OpenCVë¡œ ì²« í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("   ë¹„ë””ì˜¤ ì½”ë±ì´ OpenCVì™€ í˜¸í™˜ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("ğŸ”„ PyAVë¡œ ì „í™˜ì„ ì‹œë„í•©ë‹ˆë‹¤...")
            print("   ğŸ’¡ ì•„ë˜ 'Failed to decode frame' ê²½ê³ ëŠ” PyAV ë‚´ë¶€ ë¡œê·¸ì´ë©° ë¬´ì‹œí•´ë„ ë©ë‹ˆë‹¤.")
            cap.release()

            # PyAVë¡œ ì—´ê¸°
            cap = PyAVVideoReader(video_path)
            if not cap.open():
                raise RuntimeError(
                    "ì²« í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                    "ë¹„ë””ì˜¤ íŒŒì¼ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ PyAVê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                    "PyAV ì„¤ì¹˜: pip install av"
                )

            # PyAVì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            fps = cap.fps
            total_frames = cap.total_frames
            self.stats['total_frames'] = total_frames
            using_pyav = True

            # ì²« í”„ë ˆì„ ë‹¤ì‹œ ì½ê¸°
            ret, prev_frame = cap.read()
            if not ret:
                cap.release()
                raise RuntimeError("PyAVë¡œë„ ì²« í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        frame_idx = 0

        # Progress update ì‹œê°„ ê¸°ë°˜ ì œì–´
        import time
        last_update_time = time.time()
        UPDATE_INTERVAL = 0.1  # 0.1ì´ˆ(100ms)ë§ˆë‹¤ ì—…ë°ì´íŠ¸

        # GPU ê°€ì† ê²½ê³ 
        if self.gpu_available:
            print(f"âš ï¸ ì°¸ê³ : Optical FlowëŠ” CPU ì „ìš©ì´ë¯€ë¡œ GPU ê°€ì† íš¨ê³¼ê°€ ì œí•œì ì…ë‹ˆë‹¤.")
            print(f"   (magnitude ê³„ì‚°ë§Œ GPU ì‚¬ìš©, ì „ì²´ì˜ ~5% ë¯¸ë§Œ)")
            print(f"   ë” ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ì›í•˜ì‹œë©´ flow_scaleì„ ë‚®ì¶”ê±°ë‚˜ frame_skipì„ ë†’ì´ì„¸ìš”.")

        while True:
            # í”„ë ˆì„ ìŠ¤í‚µ ì ìš©
            for _ in range(self.config.frame_skip - 1):
                ret = cap.grab()  # í”„ë ˆì„ ì½ì§€ ì•Šê³  ê±´ë„ˆë›°ê¸°
                if not ret:
                    break
                frame_idx += 1

            ret, current_frame = cap.read()
            if not ret:
                break

            frame_idx += 1

            # ì‹œê°„ ê¸°ë°˜ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (0.1ì´ˆë§ˆë‹¤)
            current_time = time.time()
            if progress_callback and (current_time - last_update_time >= UPDATE_INTERVAL):
                progress_callback(frame_idx, total_frames)
                last_update_time = current_time

            # Optical Flow + íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ ìŠ¤ì½”ì–´ ê³„ì‚°
            motion_score, scene_change_score = self.calculate_motion_and_scene_change(prev_frame, current_frame)
            motion_buffer.append(motion_score)
            scene_change_buffer.append(scene_change_score)

            # ì¥ë©´ ì „í™˜ ê°ì§€ (scene_change_threshold ì´ìƒ)
            if scene_change_score >= self.config.scene_change_threshold:
                self.stats['scene_changes'] += 1

                # ì´ì „ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ (ì¡°ê±´ ì¶©ì¡± ì‹œ)
                if dynamic_frame_count >= self.config.min_dynamic_frames:
                    segment = self._create_segment(
                        current_segment_start,
                        frame_idx - 1,
                        fps,
                        motion_buffer[:-1],
                        scene_change_buffer[:-1]
                    )

                    if self._is_valid_segment(segment):
                        segments.append(segment)
                        # ìš°ì„ ìˆœìœ„ë³„ ì¹´ìš´íŒ…
                        if segment.priority == 1:
                            self.stats['priority_1_segments'] += 1
                        elif segment.priority == 2:
                            self.stats['priority_2_segments'] += 1
                        else:
                            self.stats['priority_3_segments'] += 1
                    else:
                        if segment.duration < self.config.min_duration:
                            self.stats['discarded_short'] += 1
                        elif segment.avg_motion < self.config.motion_low_threshold:
                            self.stats['discarded_low_motion'] += 1

                # ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘
                current_segment_start = frame_idx
                dynamic_frame_count = 0
                motion_buffer = []
                scene_change_buffer = []

            # ë™ì  êµ¬ê°„ ì¹´ìš´íŠ¸ (motion_low_threshold ì´ìƒì´ë©´ ë™ì )
            elif motion_score >= self.config.motion_low_threshold:
                dynamic_frame_count += 1

            # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ì‹œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• 
            segment_frames = frame_idx - current_segment_start
            segment_duration = segment_frames / fps

            if segment_duration >= self.config.max_duration:
                if dynamic_frame_count >= self.config.min_dynamic_frames:
                    segment = self._create_segment(
                        current_segment_start,
                        frame_idx,
                        fps,
                        motion_buffer,
                        scene_change_buffer
                    )

                    if self._is_valid_segment(segment):
                        segments.append(segment)
                        # ìš°ì„ ìˆœìœ„ë³„ ì¹´ìš´íŒ…
                        if segment.priority == 1:
                            self.stats['priority_1_segments'] += 1
                        elif segment.priority == 2:
                            self.stats['priority_2_segments'] += 1
                        else:
                            self.stats['priority_3_segments'] += 1

                current_segment_start = frame_idx
                dynamic_frame_count = 0
                motion_buffer = []
                scene_change_buffer = []

            # ìµœëŒ€ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ ë„ë‹¬
            if (self.config.max_segments and
                len(segments) >= self.config.max_segments):
                print(f"\nâš ï¸ ìµœëŒ€ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜({self.config.max_segments})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                break

            prev_frame = current_frame

        # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
        if dynamic_frame_count >= self.config.min_dynamic_frames:
            segment = self._create_segment(
                current_segment_start,
                frame_idx,
                fps,
                motion_buffer,
                scene_change_buffer
            )

            if self._is_valid_segment(segment):
                segments.append(segment)
                # ìš°ì„ ìˆœìœ„ë³„ ì¹´ìš´íŒ…
                if segment.priority == 1:
                    self.stats['priority_1_segments'] += 1
                elif segment.priority == 2:
                    self.stats['priority_2_segments'] += 1
                else:
                    self.stats['priority_3_segments'] += 1

        cap.release()

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (GPU ê°€ì† ì‚¬ìš© ì‹œ)
        if self.gpu_available and self.device and self.device.type == 'cuda':
            try:
                import torch
                print(f"\nğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")

                # 1. ìºì‹œëœ ë©”ëª¨ë¦¬ í•´ì œ
                torch.cuda.empty_cache()

                # 2. GPU ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
                torch.cuda.synchronize()

                # 3. ë©”ëª¨ë¦¬ í†µê³„ ì¶œë ¥
                memory_allocated = torch.cuda.memory_allocated(0) / 1024 / 1024  # MB
                memory_reserved = torch.cuda.memory_reserved(0) / 1024 / 1024    # MB

                print(f"   GPU ë©”ëª¨ë¦¬ í• ë‹¹: {memory_allocated:.1f} MB")
                print(f"   GPU ë©”ëª¨ë¦¬ ì˜ˆì•½: {memory_reserved:.1f} MB")
                print(f"   âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                print(f"   âš ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")

        print(f"\nâœ… ì„¸ê·¸ë¨¼íŠ¸ íƒì§€ ì™„ë£Œ!")
        self._print_stats()

        return segments

    def _create_segment(
        self,
        start_frame: int,
        end_frame: int,
        fps: float,
        motion_scores: List[float],
        scene_change_scores: List[float]
    ) -> VideoSegment:
        """ì„¸ê·¸ë¨¼íŠ¸ ê°ì²´ ìƒì„± (ìš°ì„ ìˆœìœ„ ìë™ ê³„ì‚°)"""
        start_time = start_frame / fps
        end_time = end_frame / fps
        duration = end_time - start_time
        avg_motion = np.mean(motion_scores) if motion_scores else 0.0
        avg_scene_change = np.mean(scene_change_scores) if scene_change_scores else 0.0

        # ìš°ì„ ìˆœìœ„ ê²°ì •
        # 1ìˆœìœ„: ì¥ë©´ ì „í™˜ í¬í•¨ êµ¬ê°„
        if avg_scene_change >= self.config.scene_change_important:
            priority = 1
        # 2ìˆœìœ„: ì¤‘ê°„ ë™ì  êµ¬ê°„
        elif self.config.motion_low_threshold <= avg_motion <= self.config.motion_high_threshold:
            priority = 2
        # 3ìˆœìœ„: ì €ë™ì  êµ¬ê°„
        else:
            priority = 3

        return VideoSegment(
            start_frame=start_frame,
            end_frame=end_frame,
            start_time=start_time,
            end_time=end_time,
            duration=duration,
            avg_motion=avg_motion,
            avg_scene_change=avg_scene_change,
            priority=priority
        )

    def _is_valid_segment(self, segment: VideoSegment) -> bool:
        """ì„¸ê·¸ë¨¼íŠ¸ ìœ íš¨ì„± ê²€ì¦"""
        # ìµœì†Œ ê¸¸ì´ ì²´í¬
        if segment.duration < self.config.min_duration:
            return False

        # ì €ë™ì  êµ¬ê°„ ì²´í¬ (í‰ê·  motionì´ motion_low_threshold ë¯¸ë§Œì´ë©´ ì œì™¸)
        if segment.avg_motion < self.config.motion_low_threshold:
            return False

        return True

    def export_segments(
        self,
        video_path: Path,
        segments: List[VideoSegment],
        output_dir: Path,
        progress_callback=None
    ) -> List[Path]:
        """
        ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê°œë³„ ë¹„ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥ (ffmpeg ì‚¬ìš©)

        Args:
            video_path: ì›ë³¸ ë¹„ë””ì˜¤ ê²½ë¡œ
            segments: ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°±

        Returns:
            ì €ì¥ëœ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        # ffmpeg í™•ì¸ ë° ìë™ ì„¤ì¹˜
        if not check_and_install_ffmpeg():
            raise RuntimeError(
                "ffmpegë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                "í„°ë¯¸ë„ì„ ì¬ì‹œì‘í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”."
            )

        output_dir.mkdir(parents=True, exist_ok=True)

        saved_paths = []

        print(f"\nğŸ¬ ì„¸ê·¸ë¨¼íŠ¸ ë¹„ë””ì˜¤ ìƒì„± ì¤‘ (ffmpeg)...")

        for idx, segment in enumerate(segments):
            output_path = output_dir / f"segment_{idx+1:03d}.mp4"

            # ffmpegë¡œ ë¹„ë””ì˜¤ ìë¥´ê¸° (ì¬ì¸ì½”ë”© ì—†ì´ ë¹ ë¥´ê²Œ)
            cmd = [
                'ffmpeg',
                '-i', str(video_path),
                '-ss', str(segment.start_time),
                '-to', str(segment.end_time),
                '-c', 'copy',
                '-y',  # ë®ì–´ì“°ê¸°
                str(output_path)
            ]

            try:
                subprocess.run(
                    cmd,
                    check=True,
                    capture_output=True,
                    text=True,
                    creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                )
                saved_paths.append(output_path)

                if progress_callback:
                    progress_callback(idx + 1, len(segments))

                priority_label = {1: "ì¥ë©´ì „í™˜", 2: "ì¤‘ê°„ë™ì ", 3: "ì €ë™ì "}[segment.priority]
                print(f"   âœ“ segment_{idx+1:03d}.mp4 ({segment.duration:.1f}ì´ˆ, "
                      f"Motion: {segment.avg_motion:.2f}, Scene: {segment.avg_scene_change:.2f}, "
                      f"ìš°ì„ ìˆœìœ„: {priority_label})")
            except subprocess.CalledProcessError as e:
                print(f"   âš ï¸ segment_{idx+1:03d}.mp4 ìƒì„± ì‹¤íŒ¨: {e.stderr}")

        # ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ ì €ì¥ (ì‹¤í—˜ ê¸°ëŠ¥)
        if self.config.save_discarded:
            self._export_discarded_segments(video_path, segments, output_dir)

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (GPU ê°€ì† ì‚¬ìš© ì‹œ)
        if self.gpu_available and self.device and self.device.type == 'cuda':
            try:
                import torch
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            except:
                pass

        print(f"\nâœ… {len(saved_paths)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ ì™„ë£Œ!")
        return saved_paths

    def _export_discarded_segments(
        self,
        video_path: Path,
        accepted_segments: List[VideoSegment],
        output_dir: Path
    ):
        """
        ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ì„ else í´ë”ì— ì €ì¥

        Args:
            video_path: ì›ë³¸ ë¹„ë””ì˜¤ ê²½ë¡œ
            accepted_segments: ì±„íƒëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        # ffmpeg í™•ì¸ (ì´ë¯¸ export_segmentsì—ì„œ ì²´í¬í–ˆìœ¼ë¯€ë¡œ ì¬í™•ì¸ë§Œ)
        if not check_and_install_ffmpeg():
            print("âš ï¸ ffmpegë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        # else í´ë” ìƒì„±
        else_dir = output_dir / "else"
        else_dir.mkdir(exist_ok=True)

        # ë¹„ë””ì˜¤ ì´ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        total_duration = total_frames / fps
        cap.release()

        print(f"\nğŸ“¦ ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ ì €ì¥ ì¤‘...")

        # ì±„íƒëœ êµ¬ê°„ì„ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_segments = sorted(accepted_segments, key=lambda s: s.start_time)

        # ë¹ˆ êµ¬ê°„ ì°¾ê¸°
        discarded_segments = []
        prev_end_time = 0.0

        for segment in sorted_segments:
            if segment.start_time > prev_end_time + 0.1:  # 0.1ì´ˆ ì´ìƒ ê³µë°±
                discarded_segments.append((prev_end_time, segment.start_time))
            prev_end_time = segment.end_time

        # ë§ˆì§€ë§‰ êµ¬ê°„ ì´í›„
        if prev_end_time < total_duration - 0.1:
            discarded_segments.append((prev_end_time, total_duration))

        # ë¹ˆ êµ¬ê°„ ì €ì¥
        for idx, (start_time, end_time) in enumerate(discarded_segments):
            output_path = else_dir / f"discarded_{idx+1:03d}.mp4"
            duration = end_time - start_time

            cmd = [
                'ffmpeg',
                '-i', str(video_path),
                '-ss', str(start_time),
                '-to', str(end_time),
                '-c', 'copy',
                '-y',
                str(output_path)
            ]

            try:
                subprocess.run(
                    cmd,
                    check=True,
                    capture_output=True,
                    text=True,
                    creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                )
                print(f"   âœ“ discarded_{idx+1:03d}.mp4 ({duration:.1f}ì´ˆ)")
            except subprocess.CalledProcessError as e:
                print(f"   âš ï¸ discarded_{idx+1:03d}.mp4 ìƒì„± ì‹¤íŒ¨: {e.stderr}")

        print(f"âœ… {len(discarded_segments)}ê°œ ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ ì €ì¥ ì™„ë£Œ!")

    def save_metadata(
        self,
        output_dir: Path,
        video_path: Path,
        segments: List[VideoSegment]
    ):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata = {
            'source_video': str(video_path),
            'timestamp': datetime.now().isoformat(),
            'config': {
                'motion_low_threshold': self.config.motion_low_threshold,
                'motion_high_threshold': self.config.motion_high_threshold,
                'scene_change_threshold': self.config.scene_change_threshold,
                'scene_change_important': self.config.scene_change_important,
                'min_duration': self.config.min_duration,
                'max_duration': self.config.max_duration,
                'flow_scale': self.config.flow_scale,
                'frame_skip': self.config.frame_skip,
                'use_gpu': self.config.use_gpu,
                'priority_ratios': self.config.priority_ratios,
            },
            'stats': self.stats,
            'segments': [
                {
                    'index': i + 1,
                    'filename': f"segment_{i+1:03d}.mp4",
                    'start_frame': seg.start_frame,
                    'end_frame': seg.end_frame,
                    'start_time': seg.start_time,
                    'end_time': seg.end_time,
                    'duration': seg.duration,
                    'avg_motion': seg.avg_motion,
                    'avg_scene_change': seg.avg_scene_change,
                    'priority': seg.priority
                }
                for i, seg in enumerate(segments)
            ]
        }

        metadata_path = output_dir / "segments_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

    def _print_stats(self):
        """í†µê³„ ì¶œë ¥"""
        print(f"\nğŸ“Š ì„¸ê·¸ë©˜í…Œì´ì…˜ í†µê³„:")
        print(f"   - ì´ í”„ë ˆì„: {self.stats['total_frames']:,}ê°œ")
        print(f"   - ì¥ë©´ ì „í™˜: {self.stats['scene_changes']:,}ê°œ")
        print(f"   - ìš°ì„ ìˆœìœ„ë³„ ì„¸ê·¸ë¨¼íŠ¸:")
        print(f"     Â· 1ìˆœìœ„ (ì¥ë©´ì „í™˜): {self.stats['priority_1_segments']:,}ê°œ")
        print(f"     Â· 2ìˆœìœ„ (ì¤‘ê°„ë™ì ): {self.stats['priority_2_segments']:,}ê°œ")
        print(f"     Â· 3ìˆœìœ„ (ì €ë™ì ): {self.stats['priority_3_segments']:,}ê°œ")
        print(f"   - ì œì™¸ (ì§§ìŒ): {self.stats['discarded_short']:,}ê°œ")
        print(f"   - ì œì™¸ (ì €ì›€ì§ì„): {self.stats['discarded_low_motion']:,}ê°œ")

        # Optical Flow ì„±ëŠ¥ í†µê³„
        gpu_count = self.stats['flow_gpu_count']
        cpu_count = self.stats['flow_cpu_count']
        gpu_time = self.stats['flow_gpu_time']
        cpu_time = self.stats['flow_cpu_time']

        if gpu_count > 0 or cpu_count > 0:
            print(f"\nâš¡ Optical Flow ì„±ëŠ¥ í†µê³„:")
            if gpu_count > 0:
                avg_gpu_time = (gpu_time / gpu_count) * 1000  # ms
                print(f"   - GPU Flow: {gpu_count:,}íšŒ, í‰ê·  {avg_gpu_time:.2f}ms/í”„ë ˆì„, ì´ {gpu_time:.2f}ì´ˆ")
            if cpu_count > 0:
                avg_cpu_time = (cpu_time / cpu_count) * 1000  # ms
                print(f"   - CPU Flow: {cpu_count:,}íšŒ, í‰ê·  {avg_cpu_time:.2f}ms/í”„ë ˆì„, ì´ {cpu_time:.2f}ì´ˆ")
            if gpu_count > 0 and cpu_count > 0:
                speedup = (cpu_time / cpu_count) / (gpu_time / gpu_count)
                print(f"   - GPU ê°€ì† ë°°ìœ¨: {speedup:.1f}x")


def main():
    parser = argparse.ArgumentParser(
        description="Optical Flow ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ì„¸ê·¸ë©˜í…Œì´ì…˜"
    )
    parser.add_argument(
        '--input',
        type=Path,
        required=True,
        help="ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ"
    )
    parser.add_argument(
        '--output',
        type=Path,
        required=True,
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        '--motion-low',
        type=float,
        default=2.0,
        help="ì €ë™ì  êµ¬ê°„ ì„ê³„ê°’ - ì´ë³´ë‹¤ ë‚®ìœ¼ë©´ ì œì™¸ (ê¸°ë³¸: 2.0)"
    )
    parser.add_argument(
        '--motion-high',
        type=float,
        default=15.0,
        help="ê³ ë™ì  êµ¬ê°„ ì„ê³„ê°’ - ì´ë³´ë‹¤ ë†’ìœ¼ë©´ ê³¼ë„í•œ ì›€ì§ì„ìœ¼ë¡œ ì œì™¸ (ê¸°ë³¸: 15.0)"
    )
    parser.add_argument(
        '--scene-threshold',
        type=float,
        default=0.5,
        help="ì¥ë©´ ì „í™˜ ì„ê³„ê°’ (íˆìŠ¤í† ê·¸ë¨ ì°¨ì´, ê¸°ë³¸: 0.5)"
    )
    parser.add_argument(
        '--min-duration',
        type=float,
        default=5.0,
        help="ìµœì†Œ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ ì´ˆ (ê¸°ë³¸: 5.0)"
    )
    parser.add_argument(
        '--max-duration',
        type=float,
        default=60.0,
        help="ìµœëŒ€ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ ì´ˆ (ê¸°ë³¸: 60.0)"
    )
    parser.add_argument(
        '--max-segments',
        type=int,
        default=None,
        help="ìµœëŒ€ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ (ê¸°ë³¸: ë¬´ì œí•œ)"
    )
    parser.add_argument(
        '--flow-scale',
        type=float,
        default=0.5,
        help="Optical Flow ê³„ì‚° ì‹œ í•´ìƒë„ ìŠ¤ì¼€ì¼ (0.5=2ë°° ë¹ ë¦„, 1.0=ì›ë³¸, ê¸°ë³¸: 0.5)"
    )
    parser.add_argument(
        '--frame-skip',
        type=int,
        default=1,
        help="í”„ë ˆì„ ìŠ¤í‚µ (1=ëª¨ë“  í”„ë ˆì„, 3=3í”„ë ˆì„ë§ˆë‹¤, ê¸°ë³¸: 1)"
    )
    parser.add_argument(
        '--save-discarded',
        action='store_true',
        help="ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ë„ else í´ë”ì— ì €ì¥ (ì‹¤í—˜ ê¸°ëŠ¥)"
    )
    parser.add_argument(
        '--use-gpu',
        action='store_true',
        help="GPU ê°€ì† ì‚¬ìš© (CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ)"
    )

    args = parser.parse_args()

    # ì„¤ì • ìƒì„±
    config = SegmentConfig(
        motion_low_threshold=args.motion_low,
        motion_high_threshold=args.motion_high,
        scene_change_threshold=args.scene_threshold,
        scene_change_important=args.scene_threshold,
        min_duration=args.min_duration,
        max_duration=args.max_duration,
        max_segments=args.max_segments,
        flow_scale=args.flow_scale,
        frame_skip=args.frame_skip,
        save_discarded=args.save_discarded,
        use_gpu=args.use_gpu
    )

    # ì„¸ê·¸ë©˜í„° ìƒì„±
    segmenter = VideoSegmenter(config)

    # ì§„í–‰ ìƒí™© ì½œë°±
    def progress_callback(current, total):
        print(f"   ì§„í–‰: {current:,} / {total:,} ({current / total * 100:.1f}%)", end='\r')

    try:
        # ì„¸ê·¸ë¨¼íŠ¸ íƒì§€
        segments = segmenter.detect_segments(args.input, progress_callback)

        if not segments:
            print("\nâŒ ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return 1

        # ì„¸ê·¸ë¨¼íŠ¸ ë¹„ë””ì˜¤ ìƒì„±
        saved_paths = segmenter.export_segments(
            args.input,
            segments,
            args.output,
            progress_callback
        )

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        segmenter.save_metadata(args.output, args.input, segments)

        print(f"\nâœ… ì™„ë£Œ!")
        print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output}")
        print(f"   ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(saved_paths)}ê°œ")

        total_duration = sum(seg.duration for seg in segments)
        print(f"   ì´ ê¸¸ì´: {total_duration / 60:.1f}ë¶„")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == '__main__':
    exit(main())
