#!/usr/bin/env python3
"""
SSIM ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ì„¸ê·¸ë©˜í…Œì´ì…˜
ë¹„ë””ì˜¤ë¥¼ ë™ì  ë°°ê²½ êµ¬ê°„ìœ¼ë¡œ ë¶„í• í•˜ì—¬ YOLO ê³¼ì í•© ë°©ì§€ ë° ë¼ë²¨ë§ íš¨ìœ¨ ê·¹ëŒ€í™”
UIëŠ” ê³ ì •ë˜ê³  ë°°ê²½ë§Œ ë³€í•˜ëŠ” êµ¬ê°„ ì„ íƒ

ì‚¬ìš©ë²•:
    python tools/video_segmenter.py --input datasets/raw/gameplay.mp4 \
                                     --output datasets/clips/ \
                                     --dynamic-low 0.4 \
                                     --dynamic-high 0.8 \
                                     --min-duration 5
"""

import argparse
import cv2
import numpy as np
from pathlib import Path
from typing import List, Tuple, Optional
from dataclasses import dataclass
import json
from datetime import datetime
from skimage.metrics import structural_similarity as ssim
import subprocess
import shutil
import sys
import os
import bisect
import atexit
import signal
import gc
import time
import threading
import queue
from contextlib import contextmanager


def refresh_system_path():
    """
    ì‹œìŠ¤í…œ PATH í™˜ê²½ë³€ìˆ˜ë¥¼ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ìƒˆë¡œê³ ì¹¨ (Windows ì „ìš©)
    winget ì„¤ì¹˜ í›„ í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì—ì„œ PATHë¥¼ ì¦‰ì‹œ ì‚¬ìš©í•˜ê¸° ìœ„í•¨
    """
    if sys.platform != 'win32':
        return

    try:
        import winreg
        import os

        # ì‹œìŠ¤í…œ PATH ì½ê¸°
        with winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE,
                           r'SYSTEM\CurrentControlSet\Control\Session Manager\Environment',
                           0, winreg.KEY_READ) as key:
            system_path, _ = winreg.QueryValueEx(key, 'Path')

        # ì‚¬ìš©ì PATH ì½ê¸°
        with winreg.OpenKey(winreg.HKEY_CURRENT_USER,
                           r'Environment',
                           0, winreg.KEY_READ) as key:
            try:
                user_path, _ = winreg.QueryValueEx(key, 'Path')
            except FileNotFoundError:
                user_path = ''

        # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ PATH ì—…ë°ì´íŠ¸
        combined_path = f"{user_path};{system_path}" if user_path else system_path
        os.environ['PATH'] = combined_path

    except Exception as e:
        # PATH ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ë¬´ì‹œ
        pass


def check_and_install_ffmpeg() -> bool:
    """
    ffmpeg ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸ ë° ìë™ ì„¤ì¹˜

    Returns:
        bool: ffmpegê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ True, ì„¤ì¹˜ ì‹¤íŒ¨í•˜ë©´ False
    """
    # ffmpegê°€ ì´ë¯¸ PATHì— ìˆëŠ”ì§€ í™•ì¸
    if shutil.which('ffmpeg') is not None:
        return True

    print("âš ï¸ ffmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ğŸ”§ ffmpegë¥¼ ìë™ìœ¼ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤ (winget ì‚¬ìš©)...")

    try:
        # wingetìœ¼ë¡œ ffmpeg ì„¤ì¹˜ ì‹œë„
        result = subprocess.run(
            ['winget', 'install', 'Gyan.FFmpeg', '--accept-source-agreements', '--accept-package-agreements'],
            capture_output=True,
            text=True,
            timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
        )

        if result.returncode == 0:
            print("âœ… ffmpeg ì„¤ì¹˜ ì™„ë£Œ!")

            # PATH ìƒˆë¡œê³ ì¹¨ ì‹œë„
            print("ğŸ”„ PATH í™˜ê²½ë³€ìˆ˜ ìƒˆë¡œê³ ì¹¨ ì¤‘...")
            refresh_system_path()

            # ì„¤ì¹˜ í›„ ë‹¤ì‹œ í™•ì¸
            if shutil.which('ffmpeg') is not None:
                print("âœ… ffmpegë¥¼ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
                return True
            else:
                print("   âš ï¸ ì„¤ì¹˜ëŠ” ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ PATHì—ì„œ ffmpegë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("   â„¹ï¸ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì‹œë„í•´ì£¼ì„¸ìš”:")
                print("      1. í„°ë¯¸ë„ì„ ì¬ì‹œì‘í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰")
                print("      2. ì‹œìŠ¤í…œì„ ì¬ë¶€íŒ…")
                return False
        else:
            print(f"âŒ ffmpeg ì„¤ì¹˜ ì‹¤íŒ¨")
            if result.stderr:
                print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {result.stderr[:200]}")
            print("   â„¹ï¸ ìˆ˜ë™ ì„¤ì¹˜ ë°©ë²•:")
            print("      1. í„°ë¯¸ë„ì—ì„œ 'winget install Gyan.FFmpeg' ì‹¤í–‰")
            print("      2. ë˜ëŠ” https://www.gyan.dev/ffmpeg/builds/ ì—ì„œ ë‹¤ìš´ë¡œë“œ")
            return False

    except FileNotFoundError:
        print("âŒ wingetì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   â„¹ï¸ Windows 10 1809 ì´ìƒ ë˜ëŠ” Windows 11ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   â„¹ï¸ ìˆ˜ë™ ì„¤ì¹˜: https://www.gyan.dev/ffmpeg/builds/")
        return False
    except subprocess.TimeoutExpired:
        print("âŒ ffmpeg ì„¤ì¹˜ ì‹œê°„ ì´ˆê³¼ (5ë¶„)")
        print("   â„¹ï¸ ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return False
    except Exception as e:
        print(f"âŒ ffmpeg ì„¤ì¹˜ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return False


class GPUResourceManager:
    """
    GPU ë¦¬ì†ŒìŠ¤ ìë™ ê´€ë¦¬ Context Manager
    ì‘ì—… ì™„ë£Œ/ì·¨ì†Œ/ì˜¤ë¥˜ ì‹œ ìë™ìœ¼ë¡œ VRAM ì •ë¦¬
    """
    def __init__(self, device=None):
        self.device = device
        self.is_cuda = device and device.type == 'cuda'

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ë¬´ì¡°ê±´ VRAM ì •ë¦¬ (ì •ìƒ ì¢…ë£Œ/ì˜ˆì™¸ ëª¨ë‘)"""
        if self.is_cuda:
            try:
                import torch
                print("\nğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                gc.collect()

                memory_allocated = torch.cuda.memory_allocated(0) / 1024 / 1024
                memory_reserved = torch.cuda.memory_reserved(0) / 1024 / 1024
                print(f"   GPU ë©”ëª¨ë¦¬ í• ë‹¹: {memory_allocated:.1f} MB")
                print(f"   GPU ë©”ëª¨ë¦¬ ì˜ˆì•½: {memory_reserved:.1f} MB")
                print(f"   âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                print(f"   âš ï¸ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {e}")
        return False  # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´


def extract_keyframes(video_path: Path) -> List[float]:
    """
    ë¹„ë””ì˜¤ì˜ ëª¨ë“  I-Frame(Keyframe) íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
    
    1. ê³ ì† ëª¨ë“œ (Packet Header): ì»¨í…Œì´ë„ˆì˜ íŒ¨í‚· í”Œë˜ê·¸ë§Œ í™•ì¸ (ë§¤ìš° ë¹ ë¦„)
    2. ì •ë°€ ëª¨ë“œ (Frame Decode): ì‹¤ì œ í”„ë ˆì„ ë””ì½”ë”© (ëŠë¦¼, ê³ ì† ëª¨ë“œ ì‹¤íŒ¨ ì‹œ Fallback)

    Args:
        video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

    Returns:
        Keyframe íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸ (ì´ˆ ë‹¨ìœ„, ì •ë ¬ë¨)
    """
    print("ğŸ” Keyframe ì¸ë±ì‹± ì‹œì‘ (ê³ ì† ëª¨ë“œ)...")
    
    # 1. ê³ ì† ëª¨ë“œ ì‹œë„ (Packet)
    try:
        cmd = [
            'ffprobe',
            '-select_streams', 'v:0',
            '-show_entries', 'packet=pts_time,flags',
            '-of', 'csv=print_section=0',
            str(video_path)
        ]

        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0,
            encoding='utf-8',
            errors='replace'
        )

        keyframes = []
        last_log_time = time.time()
        
        while True:
            line = process.stdout.readline()
            if not line and process.poll() is not None:
                break
            
            if not line:
                continue

            # pts_time, flags (e.g., "12.345,K__")
            parts = line.strip().split(',')
            if len(parts) >= 2:
                pts_time, flags = parts[0], parts[1]
                # 'K' í”Œë˜ê·¸ê°€ ìˆìœ¼ë©´ Keyframe
                if 'K' in flags and pts_time != 'N/A':
                    try:
                        keyframes.append(float(pts_time))
                    except ValueError:
                        continue
            
            current_time = time.time()
            if current_time - last_log_time > 5.0:
                print(f"   í‚¤í”„ë ˆì„ ì¸ë±ì‹± ì¤‘ (ê³ ì†)... ({len(keyframes)}ê°œ ë°œê²¬)", flush=True)
                last_log_time = current_time

        process.wait(timeout=60) # íŒ¨í‚· ìŠ¤ìº”ì€ ë§¤ìš° ë¹ ë¥´ë¯€ë¡œ íƒ€ì„ì•„ì›ƒ ì§§ê²Œ

        if process.returncode == 0 and keyframes:
            keyframes.sort()
            print(f"âœ… Keyframe {len(keyframes)}ê°œ ì¸ë±ì‹± ì™„ë£Œ (ê³ ì† ëª¨ë“œ)", flush=True)
            return keyframes
        
        print("âš ï¸ ê³ ì† ëª¨ë“œ ì¸ë±ì‹± ì‹¤íŒ¨ ë˜ëŠ” í‚¤í”„ë ˆì„ ì—†ìŒ. ì •ë°€ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")

    except Exception as e:
        print(f"âš ï¸ ê³ ì† ëª¨ë“œ ì˜¤ë¥˜: {e}. ì •ë°€ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        if 'process' in locals() and process:
            try:
                process.kill()
            except:
                pass

    # 2. ì •ë°€ ëª¨ë“œ (Fallback)
    return _extract_keyframes_deep_scan(video_path)


def _extract_keyframes_deep_scan(video_path: Path) -> List[float]:
    """
    ì •ë°€ ëª¨ë“œ: ffprobe frame ë””ì½”ë”©ì„ í†µí•œ Keyframe ì¶”ì¶œ (ëŠë¦¼)
    """
    print("ğŸ” Keyframe ì¸ë±ì‹± ì‹œì‘ (ì •ë°€ ëª¨ë“œ - ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")
    try:
        cmd = [
            'ffprobe',
            '-select_streams', 'v:0',
            '-show_entries', 'frame=pts_time,pict_type',
            '-of', 'csv=print_section=0',
            str(video_path)
        ]

        # Popenìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥)
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0,
            encoding='utf-8',
            errors='replace'
        )

        keyframes = []
        last_log_time = time.time()
        
        # stdout ë¼ì¸ë³„ ì½ê¸°
        while True:
            line = process.stdout.readline()
            if not line and process.poll() is not None:
                break
            
            if not line:
                continue

            parts = line.strip().split(',')
            if len(parts) >= 2:
                pts_time, pict_type = parts[0], parts[1]
                if pict_type == 'I' and pts_time != 'N/A':
                    try:
                        keyframes.append(float(pts_time))
                    except ValueError:
                        continue
            
            # 5ì´ˆë§ˆë‹¤ ì§„í–‰ ìƒí™© ë¡œê¹…
            current_time = time.time()
            if current_time - last_log_time > 5.0:
                print(f"   í‚¤í”„ë ˆì„ ì¸ë±ì‹± ì¤‘ (ì •ë°€)... ({len(keyframes)}ê°œ ë°œê²¬)", flush=True)
                last_log_time = current_time

        # íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ (300ì´ˆ)
        try:
            process.wait(timeout=300)
        except subprocess.TimeoutExpired:
            process.kill()
            print("âš ï¸ Keyframe ì¶”ì¶œ ì‹œê°„ ì´ˆê³¼ (5ë¶„), ë¶€ë¶„ ê²°ê³¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        if process.returncode != 0 and process.returncode is not None:
             # stderr ì½ê¸°
            stderr_output = process.stderr.read()
            print(f"âš ï¸ Keyframe ì¶”ì¶œ ê²½ê³  (ffprobe): {stderr_output[:200]}")

        keyframes.sort()
        print(f"âœ… Keyframe {len(keyframes)}ê°œ ì¸ë±ì‹± ì™„ë£Œ (ì •ë°€ ëª¨ë“œ)", flush=True)
        return keyframes

    except Exception as e:
        print(f"âš ï¸ Keyframe ì¶”ì¶œ ì‹¤íŒ¨: {e}, Keyframe ì •ë ¬ ë¹„í™œì„±í™”")
        return []


def snap_to_keyframe(time: float, keyframes: List[float], direction: str = 'before') -> float:
    """
    ì£¼ì–´ì§„ ì‹œê°„ì„ ê°€ì¥ ê°€ê¹Œìš´ Keyframeìœ¼ë¡œ ì •ë ¬

    Args:
        time: ëŒ€ìƒ ì‹œê°„ (ì´ˆ)
        keyframes: Keyframe íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸ (ì •ë ¬ë¨)
        direction: 'before' (ì´ì „ keyframe) ë˜ëŠ” 'after' (ë‹¤ìŒ keyframe)

    Returns:
        ì •ë ¬ëœ ì‹œê°„ (ì´ˆ)
    """
    if not keyframes:
        return time

    idx = bisect.bisect_left(keyframes, time)

    if direction == 'before':
        # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš° í•´ë‹¹ Keyframe ë°˜í™˜
        if idx < len(keyframes) and abs(keyframes[idx] - time) < 1e-5:
            return keyframes[idx]
            
        # ì´ì „ Keyframe
        if idx == 0:
            return keyframes[0]
        return keyframes[idx - 1]
    else:
        # ë‹¤ìŒ Keyframe
        if idx >= len(keyframes):
            return keyframes[-1]
        return keyframes[idx]


class PyAVVideoReader:
    """
    PyAVë¥¼ ì‚¬ìš©í•œ ë¹„ë””ì˜¤ ë¦¬ë” (OpenCVê°€ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì½”ë± ì²˜ë¦¬)
    AV1, H.265, VP9 ë“± ëª¨ë“  FFmpeg ì§€ì› ì½”ë±ì„ ë¬´ì†ì‹¤ë¡œ ì½ì„ ìˆ˜ ìˆìŒ
    """

    def __init__(self, video_path: Path):
        """
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        """
        self.video_path = video_path
        self.container = None
        self.video_stream = None
        self.fps = None
        self.total_frames = None
        self.width = None
        self.height = None
        self._frame_generator = None

    def open(self) -> bool:
        """
        ë¹„ë””ì˜¤ íŒŒì¼ ì—´ê¸°

        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            import av

            self.container = av.open(str(self.video_path))
            self.video_stream = self.container.streams.video[0]

            # ë¹„ë””ì˜¤ ì •ë³´ ì¶”ì¶œ
            self.fps = float(self.video_stream.average_rate)
            self.total_frames = self.video_stream.frames
            self.width = self.video_stream.width
            self.height = self.video_stream.height

            # total_framesê°€ 0ì´ë©´ durationìœ¼ë¡œ ì¶”ì •
            if self.total_frames == 0 and self.container.duration:
                self.total_frames = int(self.container.duration * self.fps / av.time_base)

            # í”„ë ˆì„ ì œë„ˆë ˆì´í„° ì´ˆê¸°í™”
            self._frame_generator = self.container.decode(video=0)

            print(f"âœ… PyAVë¡œ ë¹„ë””ì˜¤ ì—´ê¸° ì„±ê³µ")
            print(f"   ì½”ë±: {self.video_stream.codec_context.name}")

            return True

        except ImportError:
            print("âš ï¸ PyAVê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ì„¤ì¹˜ ë°©ë²•: pip install av")
            return False
        except Exception as e:
            print(f"âš ï¸ PyAVë¡œ ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {e}")
            return False

    def read(self):
        """
        ë‹¤ìŒ í”„ë ˆì„ ì½ê¸° (OpenCVì˜ cap.read()ì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)

        Returns:
            tuple: (success, frame_bgr) - OpenCV í˜•ì‹ì˜ BGR numpy array
        """
        try:
            frame = next(self._frame_generator)
            # BGR í¬ë§·ìœ¼ë¡œ ë³€í™˜ (OpenCV í˜¸í™˜)
            img = frame.to_ndarray(format='bgr24')
            return True, img
        except StopIteration:
            return False, None
        except Exception as e:
            return False, None

    def grab(self):
        """
        í”„ë ˆì„ì„ ê±´ë„ˆë›°ê¸° (OpenCVì˜ cap.grab()ì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤)
        PyAVëŠ” grabì„ ì§ì ‘ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ readí•˜ê³  ë²„ë¦¼

        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            frame = next(self._frame_generator)
            return True
        except StopIteration:
            return False
        except Exception as e:
            return False

    def release(self):
        """ë¦¬ì†ŒìŠ¤ í•´ì œ"""
        if self.container:
            self.container.close()
            self.container = None

    def isOpened(self) -> bool:
        """ë¹„ë””ì˜¤ê°€ ì—´ë ¤ìˆëŠ”ì§€ í™•ì¸"""
        return self.container is not None


class FrameReader(threading.Thread):
    """
    ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í”„ë ˆì„ì„ ë¯¸ë¦¬ ì½ì–´ì˜¤ëŠ” í´ë˜ìŠ¤ (I/O ë³‘ëª© í•´ê²°)
    Producer-Consumer íŒ¨í„´ ì‚¬ìš©
    """
    def __init__(self, cap, queue_size=64):
        super().__init__()
        self.cap = cap
        self.queue = queue.Queue(maxsize=queue_size)
        self.stop_event = threading.Event()
        self.daemon = True  # ë©”ì¸ ìŠ¤ë ˆë“œ ì¢…ë£Œ ì‹œ ìë™ ì¢…ë£Œ

    def run(self):
        """í”„ë ˆì„ ì½ê¸° ë£¨í”„ (Producer)"""
        while not self.stop_event.is_set():
            try:
                ret, frame = self.cap.read()
                if not ret:
                    # EOF ë„ë‹¬ ì‹œ Noneì„ ë„£ì–´ ì•Œë¦¼
                    self.queue.put((False, None))
                    break
                
                # íì— ë„£ê¸° (ê½‰ ì°¨ë©´ ëŒ€ê¸°)
                self.queue.put((True, frame))
            except Exception as e:
                print(f"âš ï¸ FrameReader ì˜¤ë¥˜: {e}")
                self.queue.put((False, None))
                break

    def read(self):
        """í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸° (Consumer)"""
        try:
            # íì—ì„œ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ 5ì´ˆ)
            return self.queue.get(timeout=5)
        except queue.Empty:
            return False, None

    def stop(self):
        """ìŠ¤ë ˆë“œ ì¢…ë£Œ ìš”ì²­"""
        self.stop_event.set()
        # í ë¹„ìš°ê¸° (ë°ë“œë½ ë°©ì§€)
        while not self.queue.empty():
            try:
                self.queue.get_nowait()
            except queue.Empty:
                break


@dataclass
class VideoSegment:
    """ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´"""
    start_frame: int
    end_frame: int
    start_time: float
    end_time: float
    duration: float
    avg_ssim: float  # êµ¬ê°„ ë‚´ í‰ê·  SSIM (ì•ˆì •ì„± ì§€í‘œ)
    intervals: Optional[List[Tuple[float, float]]] = None  # Virtual Timeline: ì—¬ëŸ¬ êµ¬ê°„ [(start, end), ...]


@dataclass
class SegmentConfig:
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„¤ì •"""
    # ëª¨ë“œ ì„¤ì •
    mode: str = "auto"                   # "auto" ë˜ëŠ” "custom"

    # ì •ì  êµ¬ê°„ ê°ì§€ (ë„ˆë¬´ ë†’ì€ SSIM = ì ìˆ˜ êµ¬ê°„)
    static_threshold: float = 0.95       # SSIMì´ ì´ë³´ë‹¤ ë†’ìœ¼ë©´ ë„ˆë¬´ ì •ì  (ì œì™¸)
    min_static_duration: float = 2.0     # ìµœì†Œ ì •ì  êµ¬ê°„ ê¸¸ì´ (ì´ˆ) - ì´ë³´ë‹¤ ì§§ì€ ì •ì  êµ¬ê°„ì€ ë¬´ì‹œ

    # ì¶œë ¥ ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì •
    target_segment_duration: float = 600.0  # ëª©í‘œ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ, ê¸°ë³¸: 10ë¶„)

    # ì„±ëŠ¥ ìµœì í™”
    ssim_scale: float = 1.0              # SSIM ê³„ì‚° ì‹œ í•´ìƒë„ ìŠ¤ì¼€ì¼ (0.25 = 4ë°° ë¹ ë¦„, ì¶œë ¥ì€ ì›ë³¸ ìœ ì§€)
    frame_skip: int = 1                  # í”„ë ˆì„ ìŠ¤í‚µ (1=ëª¨ë“  í”„ë ˆì„, 3=3í”„ë ˆì„ë§ˆë‹¤)
    use_gpu: bool = False                # GPU ê°€ì† ì‚¬ìš© (CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ)
    initial_batch_size: int = 8          # ì´ˆê¸° ë°°ì¹˜ í¬ê¸° (ë™ì  ì¡°ì •ë¨)
    max_vram_usage: float = 0.75         # ìµœëŒ€ VRAM ì‚¬ìš©ë¥  (0~1) - 75%ë¡œ í•˜í–¥ ì¡°ì •

    # ì‹¤í—˜ ê¸°ëŠ¥
    save_discarded: bool = False         # ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ë„ ë³„ë„ ì €ì¥
    enable_keyframe_snap: bool = True    # Keyframe ì •ë ¬ í™œì„±í™”

    # ì¸ì½”ë”© ì„¤ì •
    re_encode: bool = False              # ì¬ì¸ì½”ë”© ì—¬ë¶€ (True=ì¬ì¸ì½”ë”©, False=ìŠ¤íŠ¸ë¦¼ ë³µì‚¬)
    encode_quality: int = 23             # ì¸ì½”ë”© í’ˆì§ˆ (CRF, 0~51, ë‚®ì„ìˆ˜ë¡ ê³ í™”ì§ˆ)
    encode_preset: str = 'fast'          # ì¸ì½”ë”© í”„ë¦¬ì…‹ (ultrafast ~ veryslow)


class VideoSegmenter:
    """SSIM ê¸°ë°˜ ë¹„ë””ì˜¤ ì„¸ê·¸ë©˜í…Œì´ì…˜"""

    def __init__(self, config: SegmentConfig = None):
        self.config = config or SegmentConfig()
        self.stats = {
            'total_frames': 0,
            'static_segments_removed': 0,  # ì œê±°ëœ ì •ì  êµ¬ê°„ ìˆ˜
            'output_segments': 0,          # ìµœì¢… ì¶œë ¥ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜
            'ssim_gpu_count': 0,           # GPUë¡œ ê³„ì‚°í•œ SSIM íšŸìˆ˜
            'ssim_cpu_count': 0,           # CPUë¡œ ê³„ì‚°í•œ SSIM íšŸìˆ˜
            'ssim_gpu_time': 0.0,          # GPU SSIM ì´ ì‹œê°„ (ì´ˆ)
            'ssim_cpu_time': 0.0,          # CPU SSIM ì´ ì‹œê°„ (ì´ˆ)
            'batch_size_adjustments': 0,   # ë°°ì¹˜ í¬ê¸° ì¡°ì • íšŸìˆ˜
        }

        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.gpu_available = False
        self.device = None
        self.current_batch_size = self.config.initial_batch_size
        self.keyframes = []  # Keyframe íƒ€ì„ìŠ¤íƒ¬í”„ ìºì‹œ

        if self.config.use_gpu:
            self.gpu_available = self._check_gpu_available()

        # Auto ëª¨ë“œ: í•´ìƒë„ ê¸°ë°˜ ìë™ ì„¤ì •
        if self.config.mode == "auto":
            self._apply_auto_config()

        # ì´ˆê¸° ìƒíƒœ ì¶œë ¥
        self._print_initial_status()

        # atexit/signal handler ë“±ë¡ (VRAM ì •ë¦¬)
        self._register_cleanup_handlers()

    def _print_initial_status(self):
        """ì´ˆê¸° ì„¤ì • ë° ìƒíƒœ ì¶œë ¥"""
        print("=" * 60)
        print(f"ğŸ¥ ë¹„ë””ì˜¤ ì„¸ê·¸ë©˜í„° ì´ˆê¸°í™”")
        print(f"   - ëª¨ë“œ: {self.config.mode.upper()}")
        print(f"   - GPU ê°€ì†: {'í™œì„±í™”' if self.gpu_available else 'ë¹„í™œì„±í™”'}")
        if self.gpu_available:
            import torch
            print(f"     â€¢ ì¥ì¹˜: {torch.cuda.get_device_name(0)}")
            print(f"     â€¢ VRAM ì œí•œ: {self.config.max_vram_usage * 100:.0f}%")
        
        print(f"   - ì„¤ì •:")
        print(f"     â€¢ ì •ì  ì„ê³„ê°’: {self.config.static_threshold}")
        print(f"     â€¢ ìµœì†Œ ì •ì  ê¸¸ì´: {self.config.min_static_duration}ì´ˆ")
        print(f"     â€¢ ëª©í‘œ ì„¸ê·¸ë¨¼íŠ¸: {self.config.target_segment_duration}ì´ˆ")
        print(f"     â€¢ SSIM ìŠ¤ì¼€ì¼: {self.config.ssim_scale}")
        print(f"     â€¢ í”„ë ˆì„ ìŠ¤í‚µ: {self.config.frame_skip}")
        print("=" * 60, flush=True)

    def _apply_auto_config(self):
        """
        Auto ëª¨ë“œ: ë¹„ë””ì˜¤ íŠ¹ì„±ì— ë”°ë¼ ìë™ ì„¤ì •
        (ì‹¤ì œ ë¹„ë””ì˜¤ ì •ë³´ëŠ” detect_segmentsì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ê°’ë§Œ ì„¤ì •)
        """
        print("ğŸ¤– Auto ëª¨ë“œ í™œì„±í™”: ìµœì  ì„¤ì • ìë™ ì ìš©")
        # ê¸°ë³¸ ìë™ ì„¤ì •ê°’
        self.config.min_static_duration = 1.0    # 1.0ì´ˆ (ê¸°ì¡´ 2.0)
        self.config.target_segment_duration = 30.0 # 30ì´ˆ (ê¸°ì¡´ 600)
        self.config.ssim_scale = 0.25            # 0.25 (ê¸°ì¡´ 0.5) - ì„±ëŠ¥ ìµœì í™”
        self.config.frame_skip = 1               # 1 (ê¸°ì¡´ 2) - ì •í™•ë„ í–¥ìƒ

    def _register_cleanup_handlers(self):
        """atexit ë° signal handler ë“±ë¡"""
        def cleanup():
            if self.gpu_available and self.device and self.device.type == 'cuda':
                try:
                    import torch
                    torch.cuda.empty_cache()
                    gc.collect()
                except:
                    pass

        atexit.register(cleanup)

        # Windowsì—ì„œëŠ” SIGBREAK, Unixì—ì„œëŠ” SIGINT/SIGTERM
        if sys.platform == 'win32':
            try:
                signal.signal(signal.SIGBREAK, lambda sig, frame: cleanup())
            except:
                pass
        else:
            signal.signal(signal.SIGINT, lambda sig, frame: cleanup())
            signal.signal(signal.SIGTERM, lambda sig, frame: cleanup())

    def _check_gpu_available(self) -> bool:
        """
        GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (CUDA/PyTorch)

        Returns:
            bool: GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ True
        """
        try:
            # 1. PyTorch ì„¤ì¹˜ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€
            import sys
            from pathlib import Path
            import os

            # src/utils ê²½ë¡œ ì¶”ê°€ (PyTorchInstaller importìš©)
            if getattr(sys, 'frozen', False):
                # PyInstaller íŒ¨í‚¤ì§• í™˜ê²½
                utils_dir = Path(sys.executable).parent / "_internal" / "src"
            else:
                # ê°œë°œ í™˜ê²½
                script_dir = Path(__file__).parent.parent
                utils_dir = script_dir / "src"

            if utils_dir.exists() and str(utils_dir) not in sys.path:
                sys.path.insert(0, str(utils_dir))

            # 2. PyTorch ì„¤ì¹˜ í™•ì¸
            try:
                from utils.pytorch_installer import PyTorchInstaller
                installer = PyTorchInstaller.get_instance()

                if not installer.is_pytorch_installed():
                    print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    print("   GPU ê°€ì†ì„ ì‚¬ìš©í•˜ë ¤ë©´ GUIì—ì„œ 'GPU ê°€ì†' ì²´í¬ë°•ìŠ¤ë¥¼ í™œì„±í™”í•˜ì„¸ìš”.")
                    return False

                # sys.pathì— PyTorch ê²½ë¡œ ì¶”ê°€
                installer.add_to_path()

            except ImportError:
                # PyTorchInstallerë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° (ì´ì „ ë²„ì „ í˜¸í™˜ì„±)
                print("âš ï¸ PyTorchInstallerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ PyTorchë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

            # 3. PyTorch import ì‹œë„
            import torch

            if torch.cuda.is_available():
                self.device = torch.device('cuda')
                gpu_name = torch.cuda.get_device_name(0)
                print(f"âœ… GPU ê°ì§€ë¨: {gpu_name}")

                # 4. ì‹¤ì œ GPU í…ì„œ ìƒì„± ë° ì—°ì‚° í…ŒìŠ¤íŠ¸
                print("ğŸ” GPU ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì¤‘...")
                try:
                    # ì‘ì€ í–‰ë ¬ ê³±ì…ˆìœ¼ë¡œ GPU ì‘ë™ í™•ì¸
                    test_tensor = torch.randn(100, 100, device=self.device)
                    result = test_tensor @ test_tensor.T
                    torch.cuda.synchronize()  # GPU ì‘ì—… ì™„ë£Œ ëŒ€ê¸°

                    # ë©”ëª¨ë¦¬ ì •ë³´ í™•ì¸
                    memory_allocated = torch.cuda.memory_allocated(0) / 1024 / 1024  # MB
                    memory_reserved = torch.cuda.memory_reserved(0) / 1024 / 1024    # MB

                    print(f"âœ… GPU ê°€ì† í™œì„±í™” ì„±ê³µ!")
                    print(f"   - GPU ë©”ëª¨ë¦¬ í• ë‹¹: {memory_allocated:.1f} MB")
                    print(f"   - GPU ë©”ëª¨ë¦¬ ì˜ˆì•½: {memory_reserved:.1f} MB")
                    return True

                except RuntimeError as e:
                    print(f"âŒ GPU í…ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
                    print("   CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    return False
                except Exception as e:
                    print(f"âŒ GPU ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                    print("   CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                    return False
            else:
                print("âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                return False

        except ImportError:
            print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            print("   GPU ê°€ì†ì„ ì‚¬ìš©í•˜ë ¤ë©´ GUIì—ì„œ 'GPU ê°€ì†' ì²´í¬ë°•ìŠ¤ë¥¼ í™œì„±í™”í•˜ì„¸ìš”.")
            return False
        except (OSError, RuntimeError) as e:
            # DLL ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” CUDA ì´ˆê¸°í™” ì˜¤ë¥˜
            print(f"âš ï¸ PyTorch ë¡œë”© ì‹¤íŒ¨: {e}")
            print("   CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            print("   í•´ê²° ë°©ë²•:")
            print("   1. GUIì—ì„œ 'GPU ê°€ì†' ì²´í¬ë°•ìŠ¤ë¥¼ ë‹¤ì‹œ í™œì„±í™”í•˜ì„¸ìš”.")
            print("   2. PyTorch ì¬ì„¤ì¹˜: ì„¤ì • ë©”ë‰´ì—ì„œ 'PyTorch ì¬ì„¤ì¹˜' í´ë¦­")
            return False
        except Exception as e:
            # ê¸°íƒ€ ëª¨ë“  ì˜ˆì™¸
            print(f"âš ï¸ GPU ì´ˆê¸°í™” ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            print("   CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            return False

    def calculate_ssim(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """
        ë‘ ì´ë¯¸ì§€ ê°„ SSIM ê³„ì‚° (CPU ë˜ëŠ” GPU)

        ì„±ëŠ¥ ìµœì í™”: config.ssim_scale < 1.0ì´ë©´ í•´ìƒë„ ì¶•ì†Œ í›„ ê³„ì‚°
        (segment êµ¬ê°„ ê²°ì •ì—ë§Œ ì‚¬ìš©, ì¶œë ¥ì€ ì›ë³¸ í•´ìƒë„ ìœ ì§€)
        """
        import time

        # í•´ìƒë„ ì¶•ì†Œ (ì„¤ì •ëœ ê²½ìš°)
        if self.config.ssim_scale < 1.0:
            h, w = img1.shape[:2]
            new_h = int(h * self.config.ssim_scale)
            new_w = int(w * self.config.ssim_scale)
            img1 = cv2.resize(img1, (new_w, new_h), interpolation=cv2.INTER_AREA)
            img2 = cv2.resize(img2, (new_w, new_h), interpolation=cv2.INTER_AREA)

        # GPU ê°€ì† ì‚¬ìš© (PyTorch ì‚¬ìš© ê°€ëŠ¥ ì‹œ)
        if self.gpu_available:
            try:
                start_time = time.perf_counter()
                score = self._calculate_ssim_gpu(img1, img2)
                elapsed = time.perf_counter() - start_time

                self.stats['ssim_gpu_count'] += 1
                self.stats['ssim_gpu_time'] += elapsed
                return score
            except (OSError, RuntimeError, Exception) as e:
                # GPU ê³„ì‚° ì‹¤íŒ¨ ì‹œ CPUë¡œ ìë™ í´ë°±
                print(f"âš ï¸ GPU SSIM ê³„ì‚° ì‹¤íŒ¨, CPUë¡œ ì „í™˜: {e}")
                self.gpu_available = False  # ì´í›„ ëª¨ë“  ê³„ì‚°ì€ CPU ì‚¬ìš©

        # CPU ë²„ì „ (ê¸°ì¡´ ì½”ë“œ)
        start_time = time.perf_counter()
        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
        score, _ = ssim(gray1, gray2, full=True)
        elapsed = time.perf_counter() - start_time

        self.stats['ssim_cpu_count'] += 1
        self.stats['ssim_cpu_time'] += elapsed
        return score

    def _adjust_batch_size(self, error: Exception = None):
        """
        VRAM ì‚¬ìš©ëŸ‰ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ì¡°ì •

        Args:
            error: OOM ì—ëŸ¬ê°€ ë°œìƒí•œ ê²½ìš° ì „ë‹¬
        """
        if not self.gpu_available:
            return

        try:
            import torch

            # OOM ë°œìƒ ì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
            if error and isinstance(error, torch.cuda.OutOfMemoryError):
                self.current_batch_size = max(1, self.current_batch_size // 2)
                self.stats['batch_size_adjustments'] += 1
                torch.cuda.empty_cache()
                print(f"âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±, ë°°ì¹˜ í¬ê¸° ê°ì†Œ: {self.current_batch_size * 2} â†’ {self.current_batch_size}")
                return

            # ê°€ìš© VRAM ì²´í¬ ë° ë°°ì¹˜ í¬ê¸° ì¡°ì •
            free_mem, total_mem = torch.cuda.mem_get_info(0)
            free_ratio = free_mem / total_mem
            
            # 1. ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ (Proactive)
            # ë‚¨ì€ ë©”ëª¨ë¦¬ê°€ 25% ë¯¸ë§Œì´ë©´ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
            if free_ratio < (1 - self.config.max_vram_usage):
                if self.current_batch_size > 1:
                    old_size = self.current_batch_size
                    self.current_batch_size = max(1, self.current_batch_size // 2)
                    self.stats['batch_size_adjustments'] += 1
                    torch.cuda.empty_cache()  # ì¤‘ìš”: ìºì‹œ ë¹„ìš°ê¸°
                    print(f"âš ï¸ VRAM ì—¬ìœ  ë¶€ì¡± ({free_ratio*100:.1f}%), ë°°ì¹˜ í¬ê¸° ê°ì†Œ: {old_size} â†’ {self.current_batch_size}")
                return

            # 2. ë©”ëª¨ë¦¬ ì—¬ìœ  ì‹œ ë°°ì¹˜ í¬ê¸° ì¦ê°€
            # ë‚¨ì€ ë©”ëª¨ë¦¬ê°€ 60% ì´ìƒì´ê³  (ë³´ìˆ˜ì  ì ‘ê·¼) í˜„ì¬ ë°°ì¹˜ê°€ ìµœëŒ€ê°€ ì•„ë‹ˆë©´ ì¦ê°€
            if free_ratio > 0.6 and self.current_batch_size < 128:
                old_size = self.current_batch_size
                self.current_batch_size = min(128, self.current_batch_size * 2)
                if old_size != self.current_batch_size:
                    self.stats['batch_size_adjustments'] += 1
                    print(f"ğŸš€ GPU ì—¬ìœ  ë©”ëª¨ë¦¬ í™•ë³´ ({free_ratio*100:.1f}%), ë°°ì¹˜ í¬ê¸° ì¦ê°€: {old_size} â†’ {self.current_batch_size}")

        except Exception as e:
            # VRAM ì¡°ì • ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
            pass

    def _calculate_ssim_gpu_batch(self, frame_pairs: list) -> list:
        """
        GPUë¥¼ ì‚¬ìš©í•œ ë°°ì¹˜ SSIM ê³„ì‚° (PyTorch) - ì„±ëŠ¥ ìµœì í™”

        Args:
            frame_pairs: [(img1, img2), ...] í”„ë ˆì„ ìŒ ë¦¬ìŠ¤íŠ¸

        Returns:
            SSIM ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        """
        try:
            import torch
            import torch.nn.functional as F

            if not frame_pairs:
                return []

            batch_size = len(frame_pairs)

            # BGR to Grayscale & Resize (CPUì—ì„œ ë°°ì¹˜ ì²˜ë¦¬)
            gray1_list = []
            gray2_list = []

            # í•´ìƒë„ ì¶•ì†Œ (ì„¤ì •ëœ ê²½ìš°)
            target_size = None
            if self.config.ssim_scale < 1.0:
                h, w = frame_pairs[0][0].shape[:2]
                new_h = int(h * self.config.ssim_scale)
                new_w = int(w * self.config.ssim_scale)
                target_size = (new_w, new_h)

            for img1, img2 in frame_pairs:
                if target_size:
                    img1 = cv2.resize(img1, target_size, interpolation=cv2.INTER_AREA)
                    img2 = cv2.resize(img2, target_size, interpolation=cv2.INTER_AREA)
                
                gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
                gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
                gray1_list.append(gray1)
                gray2_list.append(gray2)

            # NumPy to Torch Tensor (ë°°ì¹˜)
            gray1_batch = np.stack(gray1_list)
            gray2_batch = np.stack(gray2_list)

            t1 = torch.from_numpy(gray1_batch).float().unsqueeze(1).to(self.device) / 255.0
            t2 = torch.from_numpy(gray2_batch).float().unsqueeze(1).to(self.device) / 255.0

            # SSIM ê³„ì‚° (ë°°ì¹˜)
            C1 = 0.01 ** 2
            C2 = 0.03 ** 2

            mu1 = F.avg_pool2d(t1, 11, 1, 5)
            mu2 = F.avg_pool2d(t2, 11, 1, 5)

            mu1_sq = mu1 ** 2
            mu2_sq = mu2 ** 2
            mu1_mu2 = mu1 * mu2

            sigma1_sq = F.avg_pool2d(t1 ** 2, 11, 1, 5) - mu1_sq
            sigma2_sq = F.avg_pool2d(t2 ** 2, 11, 1, 5) - mu2_sq
            sigma12 = F.avg_pool2d(t1 * t2, 11, 1, 5) - mu1_mu2

            ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \
                       ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))

            # ë°°ì¹˜ë³„ í‰ê·  SSIM ì ìˆ˜
            scores = ssim_map.mean(dim=(1, 2, 3)).cpu().tolist()
            return scores

        except Exception as e:
            # GPU ì˜¤ë¥˜ ì‹œ CPUë¡œ í´ë°± (ë‹¨ì¼ í”„ë ˆì„ ì²˜ë¦¬)
            print(f"âš ï¸ GPU ë°°ì¹˜ SSIM ê³„ì‚° ì‹¤íŒ¨, CPUë¡œ í´ë°±: {e}")
            scores = []
            for img1, img2 in frame_pairs:
                gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
                gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
                score, _ = ssim(gray1, gray2, full=True)
                scores.append(score)
            return scores

    def _calculate_ssim_gpu(self, img1: np.ndarray, img2: np.ndarray) -> float:
        """
        GPUë¥¼ ì‚¬ìš©í•œ SSIM ê³„ì‚° (PyTorch)

        Args:
            img1: ì²« ë²ˆì§¸ ì´ë¯¸ì§€ (BGR)
            img2: ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ (BGR)

        Returns:
            SSIM ì ìˆ˜
        """
        try:
            import torch
            import torch.nn.functional as F

            # BGR to Grayscale
            gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

            # NumPy to Torch Tensor
            t1 = torch.from_numpy(gray1).float().unsqueeze(0).unsqueeze(0).to(self.device) / 255.0
            t2 = torch.from_numpy(gray2).float().unsqueeze(0).unsqueeze(0).to(self.device) / 255.0

            # SSIM ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
            C1 = 0.01 ** 2
            C2 = 0.03 ** 2

            mu1 = F.avg_pool2d(t1, 11, 1, 5)
            mu2 = F.avg_pool2d(t2, 11, 1, 5)

            mu1_sq = mu1 ** 2
            mu2_sq = mu2 ** 2
            mu1_mu2 = mu1 * mu2

            sigma1_sq = F.avg_pool2d(t1 ** 2, 11, 1, 5) - mu1_sq
            sigma2_sq = F.avg_pool2d(t2 ** 2, 11, 1, 5) - mu2_sq
            sigma12 = F.avg_pool2d(t1 * t2, 11, 1, 5) - mu1_mu2

            ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \
                       ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))

            score = ssim_map.mean().item()
            return score

        except Exception as e:
            # GPU ì˜¤ë¥˜ ì‹œ CPUë¡œ í´ë°±
            print(f"âš ï¸ GPU SSIM ê³„ì‚° ì‹¤íŒ¨, CPUë¡œ í´ë°±: {e}")
            gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
            score, _ = ssim(gray1, gray2, full=True)
            return score

    def detect_segments(
        self,
        video_path: Path,
        progress_callback=None
    ) -> List[VideoSegment]:
        """
        ë¹„ë””ì˜¤ì—ì„œ ìœ íš¨ êµ¬ê°„ íƒì§€ (Virtual Timeline ë°©ì‹)

        1ë‹¨ê³„: ì •ì  êµ¬ê°„ ìŠ¤ìº” ë° ë§ˆí‚¹
        2ë‹¨ê³„: ìœ íš¨ êµ¬ê°„ ë³‘í•© (ì§§ì€ ì •ì  êµ¬ê°„ ë¬´ì‹œ)
        3ë‹¨ê³„: ëˆ„ì  ì‹œê°„ ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• 

        Args:
            video_path: ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜(current, total)

        Returns:
            VideoSegment ë¦¬ìŠ¤íŠ¸ (Virtual Timeline ê¸°ë°˜)
        """
        # Keyframe ì¸ë±ì‹± (í™œì„±í™” ì‹œ)
        if self.config.enable_keyframe_snap:
            print("ğŸ” Keyframe ì¸ë±ì‹± ì¤‘...")
            self.keyframes = extract_keyframes(video_path)

        # GPU Resource Manager ì‚¬ìš©
        with GPUResourceManager(self.device):
            # 1ë‹¨ê³„: ì •ì  êµ¬ê°„ íƒì§€
            static_intervals = self._scan_static_intervals(video_path, progress_callback)

            # 2ë‹¨ê³„: ìœ íš¨ êµ¬ê°„ ê³„ì‚°
            valid_intervals = self._compute_valid_intervals(static_intervals, video_path)

            # 3ë‹¨ê³„: Virtual Timeline ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• 
            segments = self._split_by_virtual_timeline(valid_intervals, video_path)

        return segments

    def _scan_static_intervals(
        self,
        video_path: Path,
        progress_callback=None
    ) -> List[Tuple[float, float]]:
        """
        1ë‹¨ê³„: ì •ì  êµ¬ê°„ ìŠ¤ìº”

        Returns:
            ì •ì  êµ¬ê°„ ë¦¬ìŠ¤íŠ¸ [(start_time, end_time), ...]
        """
        # 1ë‹¨ê³„: OpenCVë¡œ ì—´ê¸° ì‹œë„
        cap = cv2.VideoCapture(str(video_path))
        using_pyav = False

        if not cap.isOpened():
            print("âš ï¸ OpenCVë¡œ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ì–´ PyAVë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            cap = PyAVVideoReader(video_path)
            if not cap.open():
                raise RuntimeError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            using_pyav = True

        fps = cap.get(cv2.CAP_PROP_FPS) if not using_pyav else cap.fps
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if not using_pyav else cap.total_frames
        self.stats['total_frames'] = total_frames

        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ë¶„ì„ ì¤‘...")
        print(f"   - FPS: {fps:.2f}")
        print(f"   - ì´ í”„ë ˆì„: {total_frames:,}ê°œ")
        print(f"   - ê¸¸ì´: {total_frames / fps / 60:.1f}ë¶„")
        print(f"   - ë™ì  ë°°ì¹˜ í¬ê¸°: {self.current_batch_size} (ìë™ ì¡°ì •)")

        # 2ë‹¨ê³„: ì²« í”„ë ˆì„ ì½ê¸° ê²€ì¦
        ret, prev_frame = cap.read()
        if not ret and not using_pyav:
            print("âš ï¸ OpenCVë¡œ ì²« í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨. PyAVë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            cap.release()
            cap = PyAVVideoReader(video_path)
            if not cap.open():
                raise RuntimeError("PyAVë¡œë„ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            using_pyav = True
            fps = cap.fps
            total_frames = cap.total_frames
            self.stats['total_frames'] = total_frames
            ret, prev_frame = cap.read()

        if not ret:
            cap.release()
            raise RuntimeError("ì²« í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        frame_idx = 0
        static_intervals = []
        static_start = None
        consecutive_static_frames = 0

        import time
        last_update_time = time.time()
        UPDATE_INTERVAL = 0.1

        # GPU ë°°ì¹˜ ì²˜ë¦¬
        use_batch = self.gpu_available and self.device.type == 'cuda'
        frame_batch = []
        frame_indices = []

        # FrameReader ì‹œì‘ (I/O ë³‘ë ¬ ì²˜ë¦¬)
        reader = FrameReader(cap, queue_size=64)
        reader.start()

        try:
            while True:
                # FrameReaderì—ì„œ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
                ret, current_frame = reader.read()
                if not ret:
                    break

                frame_idx += 1

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                current_time = time.time()
                if progress_callback and (current_time - last_update_time >= UPDATE_INTERVAL):
                    progress_callback(frame_idx, total_frames)
                    last_update_time = current_time

                if use_batch:
                    frame_batch.append((prev_frame.copy(), current_frame.copy()))
                    frame_indices.append(frame_idx)

                    if len(frame_batch) >= self.current_batch_size:
                        try:
                            ssim_scores = self._calculate_ssim_gpu_batch(frame_batch)

                            for batch_idx, ssim_score in enumerate(ssim_scores):
                                if ssim_score >= self.config.static_threshold:
                                    if static_start is None:
                                        static_start = (frame_indices[batch_idx] - 1) / fps
                                    consecutive_static_frames += 1
                                else:
                                    if static_start is not None and consecutive_static_frames * self.config.frame_skip / fps >= self.config.min_static_duration:
                                        static_end = frame_indices[batch_idx] / fps
                                        static_intervals.append((static_start, static_end))
                                        self.stats['static_segments_removed'] += 1
                                    static_start = None
                                    consecutive_static_frames = 0

                            # ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì •
                            self._adjust_batch_size()

                            frame_batch.clear()
                            frame_indices.clear()

                        except Exception as e:
                            import torch
                            if isinstance(e, torch.cuda.OutOfMemoryError):
                                self._adjust_batch_size(e)
                                frame_batch.clear()
                                frame_indices.clear()
                                continue
                            else:
                                raise

                    prev_frame = current_frame
                    continue

                # CPU ë‹¨ì¼ ì²˜ë¦¬
                ssim_score = self.calculate_ssim(prev_frame, current_frame)

                if ssim_score >= self.config.static_threshold:
                    if static_start is None:
                        static_start = (frame_idx - 1) / fps
                    consecutive_static_frames += 1
                else:
                    if static_start is not None and consecutive_static_frames / fps >= self.config.min_static_duration:
                        static_end = frame_idx / fps
                        static_intervals.append((static_start, static_end))
                        self.stats['static_segments_removed'] += 1
                    static_start = None
                    consecutive_static_frames = 0

                prev_frame = current_frame

        finally:
            # FrameReader ì •ë¦¬
            reader.stop()
            reader.join(timeout=1.0)
            cap.release()

        # ë§ˆì§€ë§‰ ì •ì  êµ¬ê°„ ì²˜ë¦¬
        if static_start is not None and consecutive_static_frames / fps >= self.config.min_static_duration:
            static_end = frame_idx / fps
            static_intervals.append((static_start, static_end))
            self.stats['static_segments_removed'] += 1

        print(f"\nâœ… ì •ì  êµ¬ê°„ {len(static_intervals)}ê°œ íƒì§€ ì™„ë£Œ")
        return static_intervals

    def _compute_valid_intervals(
        self,
        static_intervals: List[Tuple[float, float]],
        video_path: Path
    ) -> List[Tuple[float, float]]:
        """
        2ë‹¨ê³„: ìœ íš¨ êµ¬ê°„ ê³„ì‚° (ì •ì  êµ¬ê°„ ì œì™¸)

        Returns:
            ìœ íš¨ êµ¬ê°„ ë¦¬ìŠ¤íŠ¸ [(start_time, end_time), ...]
        """
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        total_duration = total_frames / fps
        cap.release()

        if not static_intervals:
            return [(0.0, total_duration)]

        valid_intervals = []
        prev_end = 0.0

        for start, end in sorted(static_intervals):
            if start > prev_end + 0.1:  # 0.1ì´ˆ ì´ìƒ ì°¨ì´
                valid_intervals.append((prev_end, start))
            prev_end = end

        # ë§ˆì§€ë§‰ êµ¬ê°„
        if prev_end < total_duration - 0.1:
            valid_intervals.append((prev_end, total_duration))

        total_valid_duration = sum(end - start for start, end in valid_intervals)
        print(f"âœ… ìœ íš¨ êµ¬ê°„ {len(valid_intervals)}ê°œ (ì´ {total_valid_duration / 60:.1f}ë¶„)")

        return valid_intervals

    def _split_by_virtual_timeline(
        self,
        valid_intervals: List[Tuple[float, float]],
        video_path: Path
    ) -> List[VideoSegment]:
        """
        3ë‹¨ê³„: Virtual Timeline ê¸°ë°˜ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• 

        ëˆ„ì  ì‹œê°„ì´ target_segment_durationì— ë„ë‹¬í•  ë•Œë§ˆë‹¤ ë¶„í• 
        """
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        cap.release()

        segments = []
        accumulated_time = 0.0
        segment_intervals = []  # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ì— í¬í•¨ë  êµ¬ê°„ë“¤

        for start, end in valid_intervals:
            interval_duration = end - start

            # Keyframe ì •ë ¬
            if self.config.enable_keyframe_snap and self.keyframes:
                start = snap_to_keyframe(start, self.keyframes, 'before')
                end = snap_to_keyframe(end, self.keyframes, 'after')
                interval_duration = end - start

            # í˜„ì¬ êµ¬ê°„ì´ ë‚¨ì€ ëª©í‘œ ì‹œê°„ì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
            remaining_time = self.config.target_segment_duration - accumulated_time
            
            while interval_duration >= remaining_time:
                # í˜„ì¬ êµ¬ê°„ì„ ì˜ë¼ì„œ ì„¸ê·¸ë¨¼íŠ¸ ì™„ì„±
                split_point = start + remaining_time
                
                # Keyframe ì •ë ¬ (ìë¥´ëŠ” ì§€ì )
                if self.config.enable_keyframe_snap and self.keyframes:
                    split_point = snap_to_keyframe(split_point, self.keyframes, 'before')
                    # ë„ˆë¬´ ì§§ê²Œ ì˜ë¦¬ëŠ” ê²ƒ ë°©ì§€ (ìµœì†Œ 1ì´ˆ)
                    if split_point <= start + 1.0:
                         split_point = snap_to_keyframe(start + remaining_time, self.keyframes, 'after')
                
                # ì‹¤ì œ ì˜ë¦° ê¸¸ì´
                actual_duration = split_point - start
                
                if actual_duration > 0:
                    segment_intervals.append((start, split_point))
                    accumulated_time += actual_duration
                    
                    # ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
                    seg_start = segment_intervals[0][0]
                    seg_end = segment_intervals[-1][1]
                    segments.append(VideoSegment(
                        start_frame=int(seg_start * fps),
                        end_frame=int(seg_end * fps),
                        start_time=seg_start,
                        end_time=seg_end,
                        duration=accumulated_time,
                        avg_ssim=0.0,
                        intervals=list(segment_intervals)
                    ))
                
                # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì¤€ë¹„
                start = split_point
                interval_duration = end - start
                accumulated_time = 0.0
                segment_intervals = []
                remaining_time = self.config.target_segment_duration

            # ë‚¨ì€ êµ¬ê°„ ì¶”ê°€
            if interval_duration > 0:
                segment_intervals.append((start, end))
                accumulated_time += interval_duration

        # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸
        if segment_intervals:
            seg_start = segment_intervals[0][0]
            seg_end = segment_intervals[-1][1]
            segments.append(VideoSegment(
                start_frame=int(seg_start * fps),
                end_frame=int(seg_end * fps),
                start_time=seg_start,
                end_time=seg_end,
                duration=accumulated_time,
                avg_ssim=0.0,
                intervals=list(segment_intervals)
            ))

        self.stats['output_segments'] = len(segments)
        print(f"âœ… ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸ {len(segments)}ê°œ ìƒì„± (í‰ê·  {sum(s.duration for s in segments) / len(segments) / 60:.1f}ë¶„)")

        return segments

    def _create_segment(
        self,
        start_frame: int,
        end_frame: int,
        fps: float,
        ssim_scores: List[float]
    ) -> VideoSegment:
        """ì„¸ê·¸ë¨¼íŠ¸ ê°ì²´ ìƒì„±"""
        start_time = start_frame / fps
        end_time = end_frame / fps
        duration = end_time - start_time
        avg_ssim = np.mean(ssim_scores) if ssim_scores else 0.0

        return VideoSegment(
            start_frame=start_frame,
            end_frame=end_frame,
            start_time=start_time,
            end_time=end_time,
            duration=duration,
            avg_ssim=avg_ssim
        )

    def _is_valid_segment(self, segment: VideoSegment) -> bool:
        """ì„¸ê·¸ë¨¼íŠ¸ ìœ íš¨ì„± ê²€ì¦"""
        # ìµœì†Œ ê¸¸ì´ ì²´í¬
        if segment.duration < self.config.min_duration:
            return False

        # ì •ì  êµ¬ê°„ ì²´í¬ (í‰ê·  SSIMì´ static_threshold ì´ìƒì´ë©´ ì œì™¸)
        if segment.avg_ssim >= self.config.static_threshold:
            return False

        return True

    def export_segments(
        self,
        video_path: Path,
        segments: List[VideoSegment],
        output_dir: Path,
        progress_callback=None
    ) -> List[Path]:
        """
        ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê°œë³„ ë¹„ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥ (FFmpeg concat demuxer ì‚¬ìš©)

        Virtual Timelineì˜ ì—¬ëŸ¬ êµ¬ê°„ì„ ì´ì–´ë¶™ì—¬ ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±

        Args:
            video_path: ì›ë³¸ ë¹„ë””ì˜¤ ê²½ë¡œ
            segments: ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°±

        Returns:
            ì €ì¥ëœ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        # ffmpeg í™•ì¸ ë° ìë™ ì„¤ì¹˜
        if not check_and_install_ffmpeg():
            raise RuntimeError(
                "ffmpegë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                "í„°ë¯¸ë„ì„ ì¬ì‹œì‘í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”."
            )

        output_dir.mkdir(parents=True, exist_ok=True)
        saved_paths = []

        print(f"\nğŸ¬ ì„¸ê·¸ë¨¼íŠ¸ ë¹„ë””ì˜¤ ìƒì„± ì¤‘ (FFmpeg {'ì¬ì¸ì½”ë”©' if self.config.re_encode else 'concat demuxer'})...")

        for idx, segment in enumerate(segments):
            output_path = output_dir / f"segment_{idx+1:03d}.mp4"

            # ì¬ì¸ì½”ë”© ëª¨ë“œ
            if self.config.re_encode:
                # í•„í„° ë³µì¡ë„ ìƒì„± (ì—¬ëŸ¬ êµ¬ê°„ ë³‘í•© ì‹œ)
                filter_complex = ""
                inputs = []
                
                if segment.intervals:
                    # ì—¬ëŸ¬ êµ¬ê°„ì´ ìˆëŠ” ê²½ìš°
                    for i, (start, end) in enumerate(segment.intervals):
                        inputs.extend(['-ss', str(start), '-to', str(end), '-i', str(video_path)])
                        filter_complex += f"[{i}:v:0][{i}:a:0]"
                    
                    filter_complex += f"concat=n={len(segment.intervals)}:v=1:a=1[outv][outa]"
                    
                    cmd = [
                        'ffmpeg',
                        *inputs,
                        '-filter_complex', filter_complex,
                        '-map', '[outv]', '-map', '[outa]',
                        '-c:v', 'libx264',
                        '-crf', str(self.config.encode_quality),
                        '-preset', self.config.encode_preset,
                        '-c:a', 'aac',
                        '-b:a', '192k',
                        '-y',
                        str(output_path)
                    ]
                else:
                    # ë‹¨ì¼ êµ¬ê°„ (ì´ë¡ ìƒ intervalsê°€ ì—†ëŠ” ê²½ìš°ëŠ” ì—†ì–´ì•¼ í•˜ì§€ë§Œ ë°©ì–´ ì½”ë“œ)
                    cmd = [
                        'ffmpeg',
                        '-ss', str(segment.start_time),
                        '-to', str(segment.end_time),
                        '-i', str(video_path),
                        '-c:v', 'libx264',
                        '-crf', str(self.config.encode_quality),
                        '-preset', self.config.encode_preset,
                        '-c:a', 'aac',
                        '-b:a', '192k',
                        '-y',
                        str(output_path)
                    ]

                try:
                    subprocess.run(
                        cmd,
                        check=True,
                        capture_output=True,
                        text=True,
                        creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                    )
                    saved_paths.append(output_path)

                    if progress_callback:
                        progress_callback(idx + 1, len(segments))

                    print(f"   âœ“ segment_{idx+1:03d}.mp4 ({segment.duration:.1f}ì´ˆ, ì¬ì¸ì½”ë”© ì™„ë£Œ)")

                except subprocess.CalledProcessError as e:
                    print(f"   âš ï¸ segment_{idx+1:03d}.mp4 ìƒì„± ì‹¤íŒ¨: {e.stderr}")

            # ìŠ¤íŠ¸ë¦¼ ë³µì‚¬ ëª¨ë“œ (ê¸°ì¡´ ë°©ì‹)
            else:
                # ì—¬ëŸ¬ êµ¬ê°„ì„ concatí•´ì•¼ í•˜ëŠ” ê²½ìš°
                if segment.intervals and len(segment.intervals) > 1:
                    # concat demuxerìš© ì„ì‹œ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±
                    concat_list_path = output_dir / f"_concat_{idx+1:03d}.txt"

                    with open(concat_list_path, 'w', encoding='utf-8') as f:
                        for interval_start, interval_end in segment.intervals:
                            # ê° êµ¬ê°„ë§ˆë‹¤ file, inpoint, outpoint ì§€ì •
                            # Windows ê²½ë¡œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ (.as_posix())
                            f.write(f"file '{video_path.absolute().as_posix()}'\n")
                            f.write(f"inpoint {interval_start}\n")
                            f.write(f"outpoint {interval_end}\n")

                    # FFmpeg concat demuxer ì‹¤í–‰
                    cmd = [
                        'ffmpeg',
                        '-f', 'concat',
                        '-safe', '0',
                        '-i', str(concat_list_path),
                        '-c', 'copy',
                        '-y',
                        str(output_path)
                    ]

                    try:
                        subprocess.run(
                            cmd,
                            check=True,
                            capture_output=True,
                            text=True,
                            creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                        )
                        saved_paths.append(output_path)

                        if progress_callback:
                            progress_callback(idx + 1, len(segments))

                        print(f"   âœ“ segment_{idx+1:03d}.mp4 ({segment.duration:.1f}ì´ˆ, {len(segment.intervals)}ê°œ êµ¬ê°„ ë³‘í•©)")

                        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                        concat_list_path.unlink()

                    except subprocess.CalledProcessError as e:
                        print(f"   âš ï¸ segment_{idx+1:03d}.mp4 ìƒì„± ì‹¤íŒ¨: {e.stderr}")
                        if concat_list_path.exists():
                            concat_list_path.unlink()

                else:
                    # ë‹¨ì¼ êµ¬ê°„ì¸ ê²½ìš° ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                    cmd = [
                        'ffmpeg',
                        '-i', str(video_path),
                        '-ss', str(segment.start_time),
                        '-to', str(segment.end_time),
                        '-c', 'copy',
                        '-y',
                        str(output_path)
                    ]

                    try:
                        subprocess.run(
                            cmd,
                            check=True,
                            capture_output=True,
                            text=True,
                            creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                        )
                        saved_paths.append(output_path)

                        if progress_callback:
                            progress_callback(idx + 1, len(segments))

                        print(f"   âœ“ segment_{idx+1:03d}.mp4 ({segment.duration:.1f}ì´ˆ)")

                    except subprocess.CalledProcessError as e:
                        print(f"   âš ï¸ segment_{idx+1:03d}.mp4 ìƒì„± ì‹¤íŒ¨: {e.stderr}")

        print(f"\nâœ… {len(saved_paths)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ ì™„ë£Œ!")
        return saved_paths

    def _export_discarded_segments(
        self,
        video_path: Path,
        accepted_segments: List[VideoSegment],
        output_dir: Path
    ):
        """
        ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ì„ else í´ë”ì— ì €ì¥

        Args:
            video_path: ì›ë³¸ ë¹„ë””ì˜¤ ê²½ë¡œ
            accepted_segments: ì±„íƒëœ ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        # ffmpeg í™•ì¸ (ì´ë¯¸ export_segmentsì—ì„œ ì²´í¬í–ˆìœ¼ë¯€ë¡œ ì¬í™•ì¸ë§Œ)
        if not check_and_install_ffmpeg():
            print("âš ï¸ ffmpegë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        # else í´ë” ìƒì„±
        else_dir = output_dir / "else"
        else_dir.mkdir(exist_ok=True)

        # ë¹„ë””ì˜¤ ì´ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°
        cap = cv2.VideoCapture(str(video_path))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        total_duration = total_frames / fps
        cap.release()

        print(f"\nğŸ“¦ ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ ì €ì¥ ì¤‘...")

        # ì±„íƒëœ êµ¬ê°„ì„ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_segments = sorted(accepted_segments, key=lambda s: s.start_time)

        # ë¹ˆ êµ¬ê°„ ì°¾ê¸°
        discarded_segments = []
        prev_end_time = 0.0

        for segment in sorted_segments:
            if segment.start_time > prev_end_time + 0.1:  # 0.1ì´ˆ ì´ìƒ ê³µë°±
                discarded_segments.append((prev_end_time, segment.start_time))
            prev_end_time = segment.end_time

        # ë§ˆì§€ë§‰ êµ¬ê°„ ì´í›„
        if prev_end_time < total_duration - 0.1:
            discarded_segments.append((prev_end_time, total_duration))

        # ë¹ˆ êµ¬ê°„ ì €ì¥
        for idx, (start_time, end_time) in enumerate(discarded_segments):
            output_path = else_dir / f"discarded_{idx+1:03d}.mp4"
            duration = end_time - start_time

            cmd = [
                'ffmpeg',
                '-i', str(video_path),
                '-ss', str(start_time),
                '-to', str(end_time),
                '-c', 'copy',
                '-y',
                str(output_path)
            ]

            try:
                subprocess.run(
                    cmd,
                    check=True,
                    capture_output=True,
                    text=True,
                    creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                )
                print(f"   âœ“ discarded_{idx+1:03d}.mp4 ({duration:.1f}ì´ˆ)")
            except subprocess.CalledProcessError as e:
                print(f"   âš ï¸ discarded_{idx+1:03d}.mp4 ìƒì„± ì‹¤íŒ¨: {e.stderr}")

        print(f"âœ… {len(discarded_segments)}ê°œ ì±„íƒë˜ì§€ ì•Šì€ êµ¬ê°„ ì €ì¥ ì™„ë£Œ!")

    def save_metadata(
        self,
        output_dir: Path,
        video_path: Path,
        segments: List[VideoSegment]
    ):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata = {
            'source_video': str(video_path),
            'timestamp': datetime.now().isoformat(),
            'config': {
                'mode': self.config.mode,
                'static_threshold': self.config.static_threshold,
                'min_static_duration': self.config.min_static_duration,
                'target_segment_duration': self.config.target_segment_duration,
                'ssim_scale': self.config.ssim_scale,
                'frame_skip': self.config.frame_skip,
                'use_gpu': self.config.use_gpu,
                'enable_keyframe_snap': self.config.enable_keyframe_snap,
                're_encode': self.config.re_encode,
                'encode_quality': self.config.encode_quality,
            },
            'stats': self.stats,
            'segments': [
                {
                    'index': i + 1,
                    'filename': f"segment_{i+1:03d}.mp4",
                    'start_frame': seg.start_frame,
                    'end_frame': seg.end_frame,
                    'start_time': seg.start_time,
                    'end_time': seg.end_time,
                    'duration': seg.duration,
                    'avg_ssim': seg.avg_ssim,
                    'num_intervals': len(seg.intervals) if seg.intervals else 1
                }
                for i, seg in enumerate(segments)
            ]
        }

        metadata_path = output_dir / "segments_metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

    def _print_stats(self):
        """í†µê³„ ì¶œë ¥"""
        print(f"\nğŸ“Š ì„¸ê·¸ë©˜í…Œì´ì…˜ í†µê³„:")
        print(f"   - ì´ í”„ë ˆì„: {self.stats['total_frames']:,}ê°œ")
        print(f"   - ì œê±°ëœ ì •ì  êµ¬ê°„: {self.stats['static_segments_removed']:,}ê°œ")
        print(f"   - ìµœì¢… ì¶œë ¥ ì„¸ê·¸ë¨¼íŠ¸: {self.stats['output_segments']:,}ê°œ")

        # SSIM ì„±ëŠ¥ í†µê³„
        gpu_count = self.stats['ssim_gpu_count']
        cpu_count = self.stats['ssim_cpu_count']
        gpu_time = self.stats['ssim_gpu_time']
        cpu_time = self.stats['ssim_cpu_time']

        if gpu_count > 0 or cpu_count > 0:
            print(f"\nâš¡ SSIM ì„±ëŠ¥ í†µê³„:")
            if gpu_count > 0:
                avg_gpu_time = (gpu_time / gpu_count) * 1000  # ms
                print(f"   - GPU SSIM: {gpu_count:,}íšŒ, í‰ê·  {avg_gpu_time:.2f}ms/í”„ë ˆì„, ì´ {gpu_time:.2f}ì´ˆ")
            if cpu_count > 0:
                avg_cpu_time = (cpu_time / cpu_count) * 1000  # ms
                print(f"   - CPU SSIM: {cpu_count:,}íšŒ, í‰ê·  {avg_cpu_time:.2f}ms/í”„ë ˆì„, ì´ {cpu_time:.2f}ì´ˆ")
            if gpu_count > 0 and cpu_count > 0:
                speedup = (cpu_time / cpu_count) / (gpu_time / gpu_count)
                print(f"   - GPU ê°€ì† ë°°ìœ¨: {speedup:.1f}x")

        # ë°°ì¹˜ í¬ê¸° ì¡°ì • í†µê³„
        if self.stats['batch_size_adjustments'] > 0:
            print(f"\nğŸ”§ ë™ì  ë°°ì¹˜ ì¡°ì •:")
            print(f"   - ì¡°ì • íšŸìˆ˜: {self.stats['batch_size_adjustments']:,}íšŒ")
            print(f"   - ìµœì¢… ë°°ì¹˜ í¬ê¸°: {self.current_batch_size}")


def main():
    parser = argparse.ArgumentParser(
        description="SSIM ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë¹„ë””ì˜¤ ì„¸ê·¸ë©˜í…Œì´ì…˜"
    )
    parser.add_argument(
        '--input',
        type=Path,
        required=True,
        help="ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ"
    )
    parser.add_argument(
        '--output',
        type=Path,
        required=True,
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        '--mode',
        type=str,
        choices=['auto', 'custom'],
        default='auto',
        help="ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë“œ (auto=ìë™ ì„¤ì •, custom=ì‚¬ìš©ì ì§€ì •, ê¸°ë³¸: auto)"
    )
    parser.add_argument(
        '--static-threshold',
        type=float,
        default=0.95,
        help="ì •ì  êµ¬ê°„ ì„ê³„ê°’ - ì´ë³´ë‹¤ ë†’ìœ¼ë©´ ì ìˆ˜ êµ¬ê°„ìœ¼ë¡œ ì œì™¸ (ê¸°ë³¸: 0.95)"
    )
    parser.add_argument(
        '--min-static-duration',
        type=float,
        default=2.0,
        help="ìµœì†Œ ì •ì  êµ¬ê°„ ê¸¸ì´ ì´ˆ - ì´ë³´ë‹¤ ì§§ì€ ì •ì  êµ¬ê°„ì€ ë¬´ì‹œ (ê¸°ë³¸: 2.0)"
    )
    parser.add_argument(
        '--target-duration',
        type=float,
        default=600.0,
        help="ëª©í‘œ ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ ì´ˆ (ê¸°ë³¸: 600.0, 10ë¶„)"
    )
    parser.add_argument(
        '--ssim-scale',
        type=float,
        default=1.0,
        help="SSIM ê³„ì‚° ì‹œ í•´ìƒë„ ìŠ¤ì¼€ì¼ (0.25=4ë°° ë¹ ë¦„, 1.0=ì›ë³¸, ê¸°ë³¸: 1.0)"
    )
    parser.add_argument(
        '--frame-skip',
        type=int,
        default=1,
        help="í”„ë ˆì„ ìŠ¤í‚µ (1=ëª¨ë“  í”„ë ˆì„, 2=2í”„ë ˆì„ë§ˆë‹¤, ê¸°ë³¸: 1)"
    )
    parser.add_argument(
        '--use-gpu',
        action='store_true',
        help="GPU ê°€ì† ì‚¬ìš© (CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ)"
    )
    parser.add_argument(
        '--disable-keyframe-snap',
        action='store_true',
        help="Keyframe ì •ë ¬ ë¹„í™œì„±í™” (ê¸°ë³¸: í™œì„±í™”)"
    )
    parser.add_argument(
        '--re-encode',
        action='store_true',
        help="ì¶œë ¥ ì‹œ ì¬ì¸ì½”ë”© ìˆ˜í–‰ (ë²„ë²…ì„ ë°©ì§€, ì†ë„ ëŠë¦¼)"
    )
    parser.add_argument(
        '--quality',
        type=int,
        default=23,
        help="ì¸ì½”ë”© í’ˆì§ˆ (CRF, 0~51, ê¸°ë³¸: 23, ë‚®ì„ìˆ˜ë¡ ê³ í™”ì§ˆ)"
    )
    parser.add_argument(
        '--preset',
        type=str,
        default='fast',
        help="ì¸ì½”ë”© í”„ë¦¬ì…‹ (ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow)"
    )

    args = parser.parse_args()

    # ì„¤ì • ìƒì„±
    config = SegmentConfig(
        mode=args.mode,
        static_threshold=args.static_threshold,
        min_static_duration=args.min_static_duration,
        target_segment_duration=args.target_duration,
        ssim_scale=args.ssim_scale,
        frame_skip=args.frame_skip,
        use_gpu=args.use_gpu,
        enable_keyframe_snap=not args.disable_keyframe_snap,
        re_encode=args.re_encode,
        encode_quality=args.quality,
        encode_preset=args.preset
    )

    # ì„¸ê·¸ë©˜í„° ìƒì„±
    segmenter = VideoSegmenter(config)

    # ì§„í–‰ ìƒí™© ì½œë°±
    def progress_callback(current, total):
        print(f"   ì§„í–‰: {current:,} / {total:,} ({current / total * 100:.1f}%)", end='\r')

    try:
        # ì„¸ê·¸ë¨¼íŠ¸ íƒì§€
        segments = segmenter.detect_segments(args.input, progress_callback)

        if not segments:
            print("\nâŒ ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return 1

        # ì„¸ê·¸ë¨¼íŠ¸ ë¹„ë””ì˜¤ ìƒì„±
        saved_paths = segmenter.export_segments(
            args.input,
            segments,
            args.output,
            progress_callback
        )

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        segmenter.save_metadata(args.output, args.input, segments)

        print(f"\nâœ… ì™„ë£Œ!")
        print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output}")
        print(f"   ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(saved_paths)}ê°œ")

        total_duration = sum(seg.duration for seg in segments)
        print(f"   ì´ ê¸¸ì´: {total_duration / 60:.1f}ë¶„")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == '__main__':
    exit(main())
